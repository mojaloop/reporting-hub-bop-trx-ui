## Values configuration for Mojaloop Backend Dependencies
# helm repo add bitnami https://charts.bitnami.com/bitnami
# helm repo add cloudhut https://raw.githubusercontent.com/cloudhut/charts/master/archives
# helm install backend ./dependencies/backend
# helm upgrade backend ./dependencies/backend --install

global:

################################################################
## Kafka Backend Dependency
kafka:
  enabled: false
  ## Installation
  # https://bitnami.com/stack/kafka/helm
  # https://github.com/bitnami/charts/blob/master/bitnami/kafka
  # helm repo add bitnami https://charts.bitnami.com/bitnami
  # helm install kafka bitnami/kafka -f ./bitnami-kafka-charts.IGNORE.yaml

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## @param global.storageClass Global StorageClass for Persistent Volume(s)
  ##
  global:
    imageRegistry: ''
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    storageClass: ''

  ## @section Common parameters

  ## @param nameOverride String to partially override kafka.fullname
  ##
  nameOverride: ''
  ## @param fullnameOverride String to fully override kafka.fullname
  ##
  fullnameOverride: 'kafka'
  ## @param clusterDomain Default Kubernetes cluster domain
  ##
  clusterDomain: cluster.local
  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations: {}
  ## @param extraDeploy Array of extra objects to deploy with the release
  ##
  extraDeploy: []

  ## Enable diagnostic mode in the deployment
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the deployment
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the deployment
    ##
    args:
      - infinity

  ## @section Kafka parameters

  ## Bitnami Kafka image version
  ## ref: https://hub.docker.com/r/bitnami/kafka/tags/
  ## @param image.registry Kafka image registry
  ## @param image.repository Kafka image repository
  ## @param image.tag Kafka image tag (immutable tags are recommended)
  ## @param image.pullPolicy Kafka image pull policy
  ## @param image.pullSecrets Specify docker-registry secret names as an array
  ## @param image.debug Set to true if you would like to see extra information on logs
  ##
  image:
    registry: docker.io
    repository: bitnami/kafka
    tag: 2.8.0-debian-10-r84
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Set to true if you would like to see extra information on logs
    ##
    debug: false
  ## @param config Configuration file for Kafka. Auto-generated based on other parameters when not specified (see [below](
  ## Specify content for server.properties
  ## NOTE: This will override any KAFKA_CFG_ environment variables (including those set by the chart)
  ## The server.properties is auto-generated based on other parameters when this parameter is not specified
  ##
  ## Example:
  ## config: |-
  ##   broker.id=-1
  ##   listeners=PLAINTEXT://:9092
  ##   advertised.listeners=PLAINTEXT://KAFKA_IP:9092
  ##   num.network.threads=3
  ##   num.io.threads=8
  ##   socket.send.buffer.bytes=102400
  ##   socket.receive.buffer.bytes=102400
  ##   socket.request.max.bytes=104857600
  ##   log.dirs=/bitnami/kafka/data
  ##   num.partitions=1
  ##   num.recovery.threads.per.data.dir=1
  ##   offsets.topic.replication.factor=1
  ##   transaction.state.log.replication.factor=1
  ##   transaction.state.log.min.isr=1
  ##   log.flush.interval.messages=10000
  ##   log.flush.interval.ms=1000
  ##   log.retention.hours=168
  ##   log.retention.bytes=1073741824
  ##   log.segment.bytes=1073741824
  ##   log.retention.check.interval.ms=300000
  ##   zookeeper.connect=ZOOKEEPER_SERVICE_NAME
  ##   zookeeper.connection.timeout.ms=6000
  ##   group.initial.rebalance.delay.ms=0
  ##
  config: ''
  ## @param existingConfigmap ConfigMap with Kafka Configuration
  ## NOTE: This will override config AND any KAFKA_CFG_ environment variables.
  ##
  existingConfigmap: ''
  ## @param log4j An optional log4j.properties file to overwrite the default of the Kafka brokers.
  ## An optional log4j.properties file to overwrite the default of the Kafka brokers.
  ## See an example log4j.properties at:
  ## https://github.com/apache/kafka/blob/trunk/config/log4j.properties
  ##
  log4j: ''
  ## @param existingLog4jConfigMap The name of an existing ConfigMap containing a log4j.properties file.
  ## The name of an existing ConfigMap containing a log4j.properties file.
  ## NOTE: this will override log4j.
  ##
  existingLog4jConfigMap: ''
  ## @param heapOpts Kafka's Java Heap size
  ##
  heapOpts: -Xmx1024m -Xms1024m
  ## @param deleteTopicEnable Switch to enable topic deletion or not
  ##
  deleteTopicEnable: true
  ## @param autoCreateTopicsEnable Switch to enable auto creation of topics. Enabling auto creation of topics not recommended for production or similar environments
  ##
  autoCreateTopicsEnable: true
  ## @param logFlushIntervalMessages The number of messages to accept before forcing a flush of data to disk
  ##
  logFlushIntervalMessages: _10000
  ## @param logFlushIntervalMs The maximum amount of time a message can sit in a log before we force a flush
  ##
  logFlushIntervalMs: 1000
  ## @param logRetentionBytes A size-based retention policy for logs
  ##
  logRetentionBytes: _1073741824
  ## @param logRetentionCheckIntervalMs The interval at which log segments are checked to see if they can be deleted
  ##
  logRetentionCheckIntervalMs: 300000
  ## @param logRetentionHours The minimum age of a log file to be eligible for deletion due to age
  ##
  logRetentionHours: 168
  ## @param logSegmentBytes The maximum size of a log segment file. When this size is reached a new log segment will be created
  ##
  logSegmentBytes: _1073741824
  ## @param logsDirs A comma separated list of directories under which to store log files
  ##
  logsDirs: /bitnami/kafka/data
  ## @param maxMessageBytes The largest record batch size allowed by Kafka
  ##
  maxMessageBytes: _1000012
  ## @param defaultReplicationFactor Default replication factors for automatically created topics
  ##
  defaultReplicationFactor: 1
  ## @param offsetsTopicReplicationFactor The replication factor for the offsets topic
  ##
  offsetsTopicReplicationFactor: 1
  ## @param transactionStateLogReplicationFactor The replication factor for the transaction topic
  ##
  transactionStateLogReplicationFactor: 1
  ## @param transactionStateLogMinIsr Overridden min.insync.replicas config for the transaction topic
  ##
  transactionStateLogMinIsr: 1
  ## @param numIoThreads The number of threads doing disk I/O
  ##
  numIoThreads: 8
  ## @param numNetworkThreads The number of threads handling network requests
  ##
  numNetworkThreads: 3
  ## @param numPartitions The default number of log partitions per topic
  ##
  numPartitions: 1
  ## @param numRecoveryThreadsPerDataDir The number of threads per data directory to be used for log recovery at startup and flushing at shutdown
  ##
  numRecoveryThreadsPerDataDir: 1
  ## @param socketReceiveBufferBytes The receive buffer (SO_RCVBUF) used by the socket server
  ##
  socketReceiveBufferBytes: 102400
  ## @param socketRequestMaxBytes The maximum size of a request that the socket server will accept (protection against OOM)
  ##
  socketRequestMaxBytes: _104857600
  ## @param socketSendBufferBytes The send buffer (SO_SNDBUF) used by the socket server
  ##
  socketSendBufferBytes: 102400
  ## @param zookeeperConnectionTimeoutMs Timeout in ms for connecting to Zookeeper
  ##
  zookeeperConnectionTimeoutMs: 6000
  ## @param command Override kafka container command
  ##
  command:
    - /scripts/setup.sh
  ## @param args Override kafka container arguments
  ##
  args: []
  ## @param extraEnvVars Extra environment variables to add to kafka pods (see [below]({KEY}
  ## ref: https://github.com/bitnami/bitnami-docker-kafka#configuration
  ## Example:
  ## extraEnvVars:
  ##   - name: KAFKA_CFG_BACKGROUND_THREADS
  ##     value: "10"
  ##
  extraEnvVars: []
  ## @param extraVolumes Extra volume(s) to add to Kafka statefulset
  ## Examples:
  ## extraVolumes:
  ##   - name: kafka-jaas
  ##     secret:
  ##       secretName: kafka-jaas
  extraVolumes: []
  ## @param extraVolumeMounts Extra volumeMount(s) to add to Kafka containers
  ## extraVolumeMounts:
  ##   - name: kafka-jaas
  ##     mountPath: /bitnami/kafka/config/kafka_jaas.conf
  ##     subPath: kafka_jaas.conf
  extraVolumeMounts: []
  ## Authentication parameteres
  ## https://github.com/bitnami/bitnami-docker-kafka#security
  ##
  auth:
    ## Authentication protocol for client and inter-broker communications
    ## This table shows the security provided on each protocol:
    ## | Method    | Authentication                | Encryption via TLS |
    ## | plaintext | None                          | No                 |
    ## | tls       | None                          | Yes                |
    ## | mtls      | Yes (two-way authentication)  | Yes                |
    ## | sasl      | Yes (via SASL)                | No                 |
    ## | sasl_tls  | Yes (via SASL)                | Yes                |
    ## @param auth.clientProtocol Authentication protocol for communications with clients. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`
    ## @param auth.interBrokerProtocol Authentication protocol for inter-broker communications. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`
    ##
    clientProtocol: plaintext
    interBrokerProtocol: plaintext
    ## SASL configuration
    ##
    sasl:
      ## @param auth.sasl.mechanisms SASL mechanisms when either `auth.interBrokerProtocol` or `auth.clientProtocol` are `sasl`. Allowed types: `plain`, `scram-sha-256`, `scram-sha-512`
      ##
      mechanisms: plain,scram-sha-256,scram-sha-512
      ## @param auth.sasl.interBrokerMechanism SASL mechanism for inter broker communication.
      ##
      interBrokerMechanism: plain
      ## JAAS configuration for SASL authentication.
      ##
      jaas:
        ## @param auth.sasl.jaas.clientUsers Kafka client user list
        ##
        ## clientUsers:
        ##   - user1
        ##   - user2
        ##
        clientUsers:
          - user
        ## @param auth.sasl.jaas.clientPasswords Kafka client passwords. This is mandatory if more than one user is specified in clientUsers
        ##
        ## clientPasswords:
        ##   - password1
        ##   - password2"
        ##
        clientPasswords: []
        ## @param auth.sasl.jaas.interBrokerUser Kafka inter broker communication user for SASL authentication
        ##
        interBrokerUser: admin
        ## @param auth.sasl.jaas.interBrokerPassword Kafka inter broker communication password for SASL authentication
        ##
        interBrokerPassword: ''
        ## @param auth.sasl.jaas.zookeeperUser Kafka Zookeeper user for SASL authentication
        ##
        zookeeperUser: ''
        ## @param auth.sasl.jaas.zookeeperPassword Kafka Zookeeper password for SASL authentication
        ##
        zookeeperPassword: ''
        ## @param auth.sasl.jaas.existingSecret Name of the existing secret containing credentials for clientUsers, interBrokerUser and zookeeperUser
        ## Create this secret running the command below where SECRET_NAME is the name of the secret you want to create:
        ##       kubectl create secret generic SECRET_NAME --from-literal=client-passwords=CLIENT_PASSWORD1,CLIENT_PASSWORD2 --from-literal=inter-broker-password=INTER_BROKER_PASSWORD --from-literal=zookeeper-password=ZOOKEEPER_PASSWORD
        ##
        existingSecret: ''
    ## @param auth.saslMechanisms DEPRECATED: use `auth.sasl.mechanisms` instead.
    ##
    saslMechanisms: plain,scram-sha-256,scram-sha-512
    ## @param auth.saslInterBrokerMechanism DEPRECATED: use `auth.sasl.interBrokerMechanism` instead.
    ##
    saslInterBrokerMechanism: plain
    ## @param auth.jaas [object] DEPRECATED: use `auth.sasl.jaas` instead.
    ## @skip auth.jaas.clientUsers
    ##
    jaas:
      clientUsers:
        - user
      clientPasswords: []
      interBrokerUser: admin
      interBrokerPassword: ''
      zookeeperUser: ''
      zookeeperPassword: ''
      existingSecret: ''
    ## TLS configuration
    ##
    tls:
      ## @param auth.tls.type Format to use for TLS certificates. Allowed types: `jks` and `pem`
      ##
      type: jks
      ## @param auth.tls.existingSecret Name of the existing secret containing the TLS certificates for the Kafka brokers
      ##
      ## When using 'jks' format for certificates, the secret should contain:
      ##  - A truststore
      ##  - One keystore per Kafka broker you have in the cluster
      ## Create this secret following the steps below:
      ## 1) Generate your trustore and keystore files. Helpful script: https://raw.githubusercontent.com/confluentinc/confluent-platform-security-tools/master/kafka-generate-ssl.sh
      ## 2) Rename your truststore to `kafka.truststore.jks`.
      ## 3) Rename your keystores to `kafka-X.keystore.jks` where X is the ID of each Kafka broker.
      ## 4) Run the command below where SECRET_NAME is the name of the secret you want to create:
      ##       kubectl create secret generic SECRET_NAME --from-file=./kafka.truststore.jks --from-file=./kafka-0.keystore.jks --from-file=./kafka-1.keystore.jks ...
      ##
      ## When using 'pem' format for certificates, the secret should contain:
      ##  - A public CA certificate
      ##  - One public certificate and one private key per Kafka broker you have in the cluster
      ## Create this secret following the steps below:
      ## 1) Create a certificate key and signing request per Kafka broker, and sign the signing request with your CA
      ## 2) Rename your CA file to `kafka.truststore.pem`.
      ## 3) Rename your certificates to `kafka-X.keystore.pem` where X is the ID of each Kafka broker.
      ## 3) Rename your keys to `kafka-X.keystore.key` where X is the ID of each Kafka broker.
      ## 5) Run the command below where SECRET_NAME is the name of the secret you want to create:
      ##       kubectl create secret generic SECRET_NAME --from-file=./kafka.truststore.pem --from-file=./kafka-0.keystore.pem --from-file=./kafka-0.keystore.key --from-file=./kafka-1.keystore.pem --from-file=./kafka-1.keystore.key ...
      ##
      existingSecret: ''
      ## @param auth.tls.autoGenerated Generate automatically self-signed TLS certificates for Kafka brokers. Currently only supported if `auth.tls.type` is `pem`
      ## Note: ignored when using 'jks' format or `auth.tls.existingSecret` is not empty
      ##
      autoGenerated: false
      ## @param auth.tls.password Password to access the JKS files or PEM key when they are password-protected.
      ##
      password: ''
      ## @param auth.tls.jksTruststoreSecret Name of the existing secret containing your truststore if truststore not existing or different from the one in the `auth.tls.existingSecret`
      ## or is different from the one in the `auth.tls.existingSecret`.
      ## Note: ignored when using 'pem' format for certificates .
      ##
      jksTruststoreSecret: ''
      ## @param auth.tls.jksKeystoreSAN The secret key from the `auth.tls.existingSecret` containing the keystore with a SAN certificate
      ## The SAN certificate in it should be issued with Subject Alternative Names for all headless services:
      ##  - kafka-0.kafka-headless.kafka.svc.cluster.local
      ##  - kafka-1.kafka-headless.kafka.svc.cluster.local
      ##  - kafka-2.kafka-headless.kafka.svc.cluster.local
      ## Note: ignored when using 'pem' format for certificates.
      ##
      jksKeystoreSAN: ''
      ## @param auth.tls.jksTruststore The secret key from the `auth.tls.existingSecret` or `auth.tls.jksTruststoreSecret` containing the truststore
      ## Note: ignored when using 'pem' format for certificates.
      ##
      jksTruststore: ''
      ## @param auth.tls.endpointIdentificationAlgorithm The endpoint identification algorithm to validate server hostname using server certificate
      ## Disable server host name verification by setting it to an empty string.
      ## ref: https://docs.confluent.io/current/kafka/authentication_ssl.html#optional-settings
      ##
      endpointIdentificationAlgorithm: https
    ## @param auth.jksSecret DEPRECATED: use `auth.tls.existingSecret` instead.
    ##
    jksSecret: ''
    ## @param auth.jksTruststoreSecret DEPRECATED: use `auth.tls.jksTruststoreSecret` instead.
    ##
    jksTruststoreSecret: ''
    ## @param auth.jksKeystoreSAN DEPRECATED: use `auth.tls.jksKeystoreSAN` instead.
    ##
    jksKeystoreSAN: ''
    ## @param auth.jksTruststore DEPRECATED: use `auth.tls.jksTruststore` instead.
    ##
    jksTruststore: ''
    ## @param auth.jksPassword DEPRECATED: use `auth.tls.password` instead.
    ##
    jksPassword: ''
    ## @param auth.tlsEndpointIdentificationAlgorithm DEPRECATED: use `auth.tls.endpointIdentificationAlgorithm` instead.
    ##
    tlsEndpointIdentificationAlgorithm: https
  ## @param listeners The address(es) the socket server listens on. Auto-calculated it's set to an empty array
  ## When it's set to an empty array, the listeners will be configured
  ## based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)
  ##
  listeners: []
  ## @param advertisedListeners The address(es) (hostname:port) the broker will advertise to producers and consumers. Auto-calculated it's set to an empty array
  ## When it's set to an empty array, the advertised listeners will be configured
  ## based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)
  ##
  advertisedListeners: []
  ## @param listenerSecurityProtocolMap The protocol->listener mapping. Auto-calculated it's set to nil
  ## When it's nil, the listeners will be configured based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)
  ##
  listenerSecurityProtocolMap: ''
  ## @param allowPlaintextListener Allow to use the PLAINTEXT listener
  ##
  allowPlaintextListener: true
  ## @param interBrokerListenerName The listener that the brokers should communicate on
  ##
  interBrokerListenerName: INTERNAL

  ## @section Statefulset parameters

  ## @param replicaCount Number of Kafka nodes
  ##
  replicaCount: 1
  ## @param minBrokerId Minimal broker.id value, nodes increment their `broker.id` respectively
  ## Brokers increment their ID starting at this minimal value.
  ## E.g., with `minBrokerId=100` and 3 nodes, IDs will be 100, 101, 102 for brokers 0, 1, and 2, respectively.
  ##
  minBrokerId: 0
  ## @param updateStrategy Update strategy for the stateful set
  ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets
  ##
  updateStrategy: RollingUpdate
  ## @param rollingUpdatePartition Partition update strategy
  ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
  ##
  rollingUpdatePartition: ''
  ## @param hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param podManagementPolicy StatefulSet controller supports relax its ordering guarantees while preserving its uniqueness and identity guarantees. There are two valid pod management policies: OrderedReady and Parallel
  ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
  ##
  podManagementPolicy: Parallel
  ## @param schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ''
  ## @param podLabels Kafka pod labels
  ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param podAnnotations Kafka Pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param priorityClassName Name of the existing priority class to be used by kafka pods
  ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  ##
  priorityClassName: ''
  ## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ''
  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ''
    ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ''
    ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: {}
  ## @param terminationGracePeriodSeconds Seconds the pod needs to gracefully terminate
  ## ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-execution
  ##
  terminationGracePeriodSeconds: ''
  ## Kafka pods' Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param podSecurityContext.enabled Enable security context for the pods
  ## @param podSecurityContext.fsGroup Group ID for the filesystem used by the containers
  ## @param podSecurityContext.runAsUser User ID for the service user running the pod
  ##
  podSecurityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  ## @param containerSecurityContext Kafka containers' Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## Example:
  ##   containerSecurityContext:
  ##     capabilities:
  ##       drop: ["NET_RAW"]
  ##     readOnlyRootFilesystem: true
  ##
  containerSecurityContext: {}
  ## Kafka containers' resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param resources.limits The resources limits for Kafka containers
  ## @param resources.requests The requested resources for Kafka containers
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 250m
    ##    memory: 1Gi
    limits: {}
    ## Examples:
    ## requests:
    ##    cpu: 250m
    ##    memory: 256Mi
    requests: {}
  ## Kafka containers' liveness probe. Evaluated as a template.
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param livenessProbe.enabled Enable livenessProbe
  ## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    periodSeconds: 10
    successThreshold: 1
  ## Kafka containers' readiness probe. Evaluated as a template.
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param readinessProbe.enabled Enable readinessProbe
  ## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    failureThreshold: 6
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1
  ## @param customLivenessProbe Custom Liveness probe configuration for Kafka
  ##
  customLivenessProbe: {}
  ## @param customReadinessProbe Custom Readiness probe configuration for Kafka
  ##
  customReadinessProbe: {}
  ## Pod Disruption Budget configuration
  ## The PDB will only be created if replicaCount is greater than 1
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions
  ##
  pdb:
    ## @param pdb.create Enable/disable a Pod Disruption Budget creation
    ##
    create: false
    ## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ##
    minAvailable: ''
    ## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    maxUnavailable: 1
  ## @param sidecars Attach additional sidecar containers to the Kafka pod
  ## Example:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param initContainers Add extra init containers
  ##
  initContainers: []

  ## @section Exposure parameters

  ## Service parameters
  ##
  service:
    ## @param service.type Kubernetes Service type
    ##
    type: ClusterIP
    ## @param service.port Kafka port for client connections
    ##
    port: 9092
    ## @param service.internalPort Kafka port for inter-broker connections
    ##
    internalPort: 9093
    ## @param service.externalPort Kafka port for external connections
    ##
    externalPort: 9094
    ## @param service.nodePorts [object] Specify the nodePort value for the LoadBalancer and NodePort service types.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePorts:
      client: ''
      external: ''
    ## @param service.loadBalancerIP loadBalancerIP for Kafka Service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    loadBalancerIP: ''
    ## @param service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## Example:
    ## loadBalancerSourceRanges:
    ## - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.annotations Service annotations
    ##
    annotations: {}
  ## External Access to Kafka brokers configuration
  ##
  externalAccess:
    ## @param externalAccess.enabled Enable Kubernetes external cluster access to Kafka brokers
    ##
    enabled: false
    ## External IPs auto-discovery configuration
    ## An init container is used to auto-detect LB IPs or node ports by querying the K8s API
    ## Note: RBAC might be required
    ##
    autoDiscovery:
      ## @param externalAccess.autoDiscovery.enabled Enable using an init container to auto-detect external IPs/ports by querying the K8s API
      ##
      enabled: false
      ## Bitnami Kubectl image
      ## ref: https://hub.docker.com/r/bitnami/kubectl/tags/
      ## @param externalAccess.autoDiscovery.image.registry Init container auto-discovery image registry
      ## @param externalAccess.autoDiscovery.image.repository Init container auto-discovery image repository
      ## @param externalAccess.autoDiscovery.image.tag Init container auto-discovery image tag (immutable tags are recommended)
      ## @param externalAccess.autoDiscovery.image.pullPolicy Init container auto-discovery image pull policy
      ## @param externalAccess.autoDiscovery.image.pullSecrets Init container auto-discovery image pull secrets
      ##
      image:
        registry: docker.io
        repository: bitnami/kubectl
        tag: 1.19.14-debian-10-r14
        ## Specify a imagePullPolicy
        ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
        ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
        ##
        pullPolicy: IfNotPresent
        ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
        ## Example:
        ## pullSecrets:
        ##   - myRegistryKeySecretName
        ##
        pullSecrets: []
      ## Init Container resource requests and limits
      ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ## We usually recommend not to specify default resources and to leave this as a conscious
      ## choice for the user. This also increases chances charts run on environments with little
      ## resources, such as Minikube. If you do want to specify resources, uncomment the following
      ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      ## @param externalAccess.autoDiscovery.resources.limits Init container auto-discovery resource limits
      ## @param externalAccess.autoDiscovery.resources.requests Init container auto-discovery resource requests
      ##
      resources:
        ## Example:
        ## limits:
        ##    cpu: 100m
        ##    memory: 128Mi
        limits: {}
        ## Examples:
        ## requests:
        ##    cpu: 100m
        ##    memory: 128Mi
        requests: {}
    ## Parameters to configure K8s service(s) used to externally access Kafka brokers
    ## A new service per broker will be created
    ##
    service:
      ## @param externalAccess.service.type Kubernetes Service type for external access. It can be NodePort or LoadBalancer
      ##
      type: LoadBalancer
      ## @param externalAccess.service.port Kafka port used for external access when service type is LoadBalancer
      ##
      port: 9094
      ## @param externalAccess.service.loadBalancerIPs Array of load balancer IPs for each Kafka broker. Length must be the same as replicaCount
      ## Example:
      ## loadBalancerIPs:
      ##   - X.X.X.X
      ##   - Y.Y.Y.Y
      ##
      loadBalancerIPs: []
      ## @param externalAccess.service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## Example:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param externalAccess.service.nodePorts Array of node ports used for each Kafka broker. Length must be the same as replicaCount
      ## Example:
      ## nodePorts:
      ##   - 30001
      ##   - 30002
      ##
      nodePorts: []
      ## @param externalAccess.service.useHostIPs Use service host IPs to configure Kafka external listener when service type is NodePort
      ##
      useHostIPs: false
      ## @param externalAccess.service.domain Domain or external ip used to configure Kafka external listener when service type is NodePort
      ## If not specified, the container will try to get the kubernetes node external IP
      ##
      domain: ''
      ## @param externalAccess.service.annotations Service annotations for external access
      ##
      annotations: {}

  ## @section Persistence parameters

  ## Persistence parameters
  ##
  persistence:
    ## @param persistence.enabled Enable Kafka data persistence using PVC, note that Zookeeper persistence is unaffected
    ##
    enabled: false
    ## @param persistence.existingClaim Provide an existing `PersistentVolumeClaim`, the value is evaluated as a template
    ## If defined, PVC must be created manually before volume will be bound
    ## The value is evaluated as a template
    ##
    existingClaim: ''
    ## @param persistence.storageClass PVC Storage Class for Kafka data volume
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ## set, choosing the default provisioner.
    ##
    storageClass: ''
    ## @param persistence.accessModes PV Access Mode
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size PVC Storage Request for Kafka data volume
    ##
    size: 8Gi
    ## @param persistence.annotations Annotations for the PVC
    ##
    annotations: {}
    ## @param persistence.selector Selector to match an existing Persistent Volume for Kafka's data PVC. If set, the PVC can't have a PV dynamically provisioned for it
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    selector: {}
    ## @param persistence.mountPath Mount path of the Kafka data volume
    ##
    mountPath: /bitnami/kafka
  ## Log Persistence parameters
  ##
  logPersistence:
    ## @param logPersistence.enabled Enable Kafka logs persistence using PVC, note that Zookeeper persistence is unaffected
    ##
    enabled: false
    ## @param logPersistence.existingClaim A manually managed Persistent Volume and Claim
    ## If defined, PVC must be created manually before volume will be bound
    ## The value is evaluated as a template
    ##
    existingClaim: ''
    ## @param logPersistence.existingLogClaim PV Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ## set, choosing the default provisioner.
    existingLogClaim: ''
    ## @param logPersistence.accessModes PV Access Mode
    ##
    accessModes:
      - ReadWriteOnce
    ## @param logPersistence.size PVC Storage Request for Kafka logs volume
    ##
    size: 8Gi
    ## @param logPersistence.annotations Annotations for the PVC
    ##
    annotations: {}
    ## @param logPersistence.selector Selector to match an existing Persistent Volume for Kafka's log data PVC. If set, the PVC can't have a PV dynamically provisioned for it
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    selector: {}
    ## @param logPersistence.mountPath Mount path of the Kafka logs volume
    ##
    mountPath: /opt/bitnami/kafka/logs

  ## @section RBAC parameters

  ## Kafka pods ServiceAccount
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable creation of ServiceAccount for Kafka pods
    ##
    create: true
    ## @param serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated
    ## If not set and create is true, a name is generated using the kafka.serviceAccountName template
    ##
    name: ''
    ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: true
  ## Role Based Access
  ## ref: https://kubernetes.io/docs/admin/authorization/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ## binding Kafka ServiceAccount to a role
    ## that allows Kafka pods querying the K8s API
    ##
    create: false

  ## @section Volume Permissions parameters

  ## Init Container parameters
  ## Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each component
  ## values from the securityContext section of the component
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`
    ##
    enabled: false
    ## The security context for the volumePermissions init container
    ## @param volumePermissions.securityContext.runAsUser User ID for the container
    ##
    securityContext:
      runAsUser: 0
    ## @param volumePermissions.image.registry Init container volume-permissions image registry
    ## @param volumePermissions.image.repository Init container volume-permissions image name
    ## @param volumePermissions.image.tag Init container volume-permissions image tag
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r173
      ## Specify a imagePullPolicy
      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
      ##
      pullPolicy: Always
      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## Example:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## Init Container resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param volumePermissions.resources.limits Init container volume-permissions resource  limits
    ## @param volumePermissions.resources.requests Init container volume-permissions resource  requests
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 100m
      ##    memory: 128Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 100m
      ##    memory: 128Mi
      requests: {}

  ## @section Metrics parameters

  ## Prometheus Exporters / Metrics
  ##
  metrics:
    ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter
    ##
    kafka:
      ## @param metrics.kafka.enabled Whether or not to create a standalone Kafka exporter to expose Kafka metrics
      ##
      enabled: false
      ## Bitnami Kafka exporter image
      ## ref: https://hub.docker.com/r/bitnami/kafka-exporter/tags/
      ## @param metrics.kafka.image.registry Kafka exporter image registry
      ## @param metrics.kafka.image.repository Kafka exporter image repository
      ## @param metrics.kafka.image.tag Kafka exporter image tag (immutable tags are recommended)
      ## @param metrics.kafka.image.pullPolicy Kafka exporter image pull policy
      ## @param metrics.kafka.image.pullSecrets Specify docker-registry secret names as an array
      ##
      image:
        registry: docker.io
        repository: bitnami/kafka-exporter
        tag: 1.3.1-debian-10-r88
        ## Specify a imagePullPolicy
        ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
        ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
        ##
        pullPolicy: IfNotPresent
        ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
        ## Example:
        ## pullSecrets:
        ##   - myRegistryKeySecretName
        ##
        pullSecrets: []
      ## @param metrics.kafka.schedulerName Name of the k8s scheduler (other than default) for Kafka Exporter
      ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
      ##
      schedulerName: ''
      ## @param metrics.kafka.extraFlags Extra flags to be passed to Kafka exporter
      ## Example:
      ## extraFlags:
      ##   tls.insecure-skip-tls-verify: ""
      ##   web.telemetry-path: "/metrics"
      ##
      extraFlags: {}
      ## @param metrics.kafka.certificatesSecret Name of the existing secret containing the optional certificate and key files
      ## for Kafka Exporter client authentication
      ##
      certificatesSecret: ''
      ## @param metrics.kafka.tlsCert The secret key from the certificatesSecret if 'client-cert' key different from the default (cert-file)
      ##
      tlsCert: cert-file
      ## @param metrics.kafka.tlsKey The secret key from the certificatesSecret if 'client-key' key different from the default (key-file)
      ##
      tlsKey: key-file
      ## @param metrics.kafka.tlsCaSecret Name of the existing secret containing the optional ca certificate for Kafka Exporter client authentication
      ##
      tlsCaSecret: ''
      ## @param metrics.kafka.tlsCaCert The secret key from the certificatesSecret or tlsCaSecret if 'ca-cert' key different from the default (ca-file)
      ##
      tlsCaCert: ca-file
      ## Prometheus Kafka Exporter' resource requests and limits
      ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ## We usually recommend not to specify default resources and to leave this as a conscious
      ## choice for the user. This also increases chances charts run on environments with little
      ## resources, such as Minikube. If you do want to specify resources, uncomment the following
      ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      ## @param metrics.kafka.resources.limits Kafka Exporter container resource limits
      ## @param metrics.kafka.resources.requests Kafka Exporter container resource requests
      ##
      resources:
        ## Example:
        ## limits:
        ##    cpu: 100m
        ##    memory: 128Mi
        limits: {}
        ## Examples:
        ## requests:
        ##    cpu: 100m
        ##    memory: 128Mi
        requests: {}
      ## @param metrics.kafka.affinity Affinity for Kafka Exporter pod assignment
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
      ##
      affinity: {}
      ## @param metrics.kafka.nodeSelector Node labels for Kafka Exporter pod assignment
      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}
      ## @param metrics.kafka.tolerations Tolerations for Kafka Exporter pod assignment
      ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      ## @param metrics.kafka.initContainers Add init containers to the Kafka exporter pods
      ## Example:
      ## initContainers:
      ##   - name: your-image-name
      ##     image: your-image
      ##     imagePullPolicy: Always
      ##     ports:
      ##       - name: portname
      ##         containerPort: 1234
      ##
      initContainers: []
      ## Service configuration
      ##
      service:
        ## @param metrics.kafka.service.type Kubernetes service type (`ClusterIP`, `NodePort` or `LoadBalancer`) for Kafka Exporter
        ##
        type: ClusterIP
        ## @param metrics.kafka.service.port Kafka Exporter Prometheus port
        ##
        port: 9308
        ## @param metrics.kafka.service.nodePort Kubernetes HTTP node port
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
        ##
        nodePort: ''
        ## @param metrics.kafka.service.loadBalancerIP loadBalancerIP if service type is `LoadBalancer`
        ## Set the LoadBalancer service type to internal only
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
        ##
        loadBalancerIP: ''
        ## @param metrics.kafka.service.loadBalancerSourceRanges Load Balancer sources
        ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
        ## Example:
        ## loadBalancerSourceRanges:
        ## - 10.10.10.0/24
        ##
        loadBalancerSourceRanges: []
        ## @param metrics.kafka.service.clusterIP Static clusterIP or None for headless services
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address
        ##
        clusterIP: ''
        ## @param metrics.kafka.service.annotations [object] Annotations for the Kafka Exporter Prometheus metrics service
        ##
        annotations:
          prometheus.io/scrape: 'true'
          prometheus.io/port: '{{ .Values.metrics.kafka.service.port }}'
          prometheus.io/path: '/metrics'
    ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics
    ##
    jmx:
      ## @param metrics.jmx.enabled Whether or not to expose JMX metrics to Prometheus
      ##
      enabled: false
      ## Bitnami JMX exporter image
      ## ref: https://hub.docker.com/r/bitnami/jmx-exporter/tags/
      ## @param metrics.jmx.image.registry JMX exporter image registry
      ## @param metrics.jmx.image.repository JMX exporter image repository
      ## @param metrics.jmx.image.tag JMX exporter image tag (immutable tags are recommended)
      ## @param metrics.jmx.image.pullPolicy JMX exporter image pull policy
      ## @param metrics.jmx.image.pullSecrets Specify docker-registry secret names as an array
      ##
      image:
        registry: docker.io
        repository: bitnami/jmx-exporter
        tag: 0.16.1-debian-10-r41
        ## Specify a imagePullPolicy
        ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
        ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
        ##
        pullPolicy: IfNotPresent
        ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
        ## Example:
        ## pullSecrets:
        ##   - myRegistryKeySecretName
        ##
        pullSecrets: []
      ## Prometheus JMX Exporter' resource requests and limits
      ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ## We usually recommend not to specify default resources and to leave this as a conscious
      ## choice for the user. This also increases chances charts run on environments with little
      ## resources, such as Minikube. If you do want to specify resources, uncomment the following
      ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      ## @param metrics.jmx.resources.limits JMX Exporter container resource limits
      ## @param metrics.jmx.resources.requests JMX Exporter container resource requests
      ##
      resources:
        ## Example:
        ## limits:
        ##    cpu: 100m
        ##    memory: 128Mi
        limits: {}
        ## Examples:
        ## requests:
        ##    cpu: 100m
        ##    memory: 128Mi
        requests: {}
      ## Service configuration
      ##
      service:
        ## @param metrics.jmx.service.type Kubernetes service type (`ClusterIP`, `NodePort` or `LoadBalancer`) for JMX Exporter
        ##
        type: ClusterIP
        ## @param metrics.jmx.service.port JMX Exporter Prometheus port
        ##
        port: 5556
        ## @param metrics.jmx.service.nodePort Kubernetes HTTP node port
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
        ##
        nodePort: ''
        ## @param metrics.jmx.service.loadBalancerIP loadBalancerIP if service type is `LoadBalancer`
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
        ##
        loadBalancerIP: ''
        ## @param metrics.jmx.service.loadBalancerSourceRanges Load Balancer sources
        ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
        ## Example:
        ## loadBalancerSourceRanges:
        ## - 10.10.10.0/24
        ##
        loadBalancerSourceRanges: []
        ## @param metrics.jmx.service.clusterIP Static clusterIP or None for headless services
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address
        ##
        clusterIP: ''
        ## @param metrics.jmx.service.annotations [object] Annotations for the JMX Exporter Prometheus metrics service
        ##
        annotations:
          prometheus.io/scrape: 'true'
          prometheus.io/port: '{{ .Values.metrics.jmx.service.port }}'
          prometheus.io/path: '/'
      ## @param metrics.jmx.whitelistObjectNames Allows setting which JMX objects you want to expose to via JMX stats to JMX Exporter
      ## Only whitelisted values will be exposed via JMX Exporter. They must also be exposed via Rules. To expose all metrics
      ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`
      ## (2) commented out above `overrideConfig`.
      ##
      whitelistObjectNames:
        - kafka.controller:*
        - kafka.server:*
        - java.lang:*
        - kafka.network:*
        - kafka.log:*
      ## @param metrics.jmx.config [string] Configuration file for JMX exporter
      ## Specify content for jmx-kafka-prometheus.yml. Evaluated as a template
      ##
      ## Credits to the incubator/kafka chart for the JMX configuration.
      ## https://github.com/helm/charts/tree/master/incubator/kafka
      ##
      config: |-
        jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
        lowercaseOutputName: true
        lowercaseOutputLabelNames: true
        ssl: false
        {{- if .Values.metrics.jmx.whitelistObjectNames }}
        whitelistObjectNames: ["{{ join "\",\"" .Values.metrics.jmx.whitelistObjectNames }}"]
        {{- end }}
      ## @param metrics.jmx.existingConfigmap Name of existing ConfigMap with JMX exporter configuration
      ## NOTE: This will override metrics.jmx.config
      ##
      existingConfigmap: ''
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled if `true`, creates a Prometheus Operator ServiceMonitor (requires `metrics.kafka.enabled` or `metrics.jmx.enabled` to be `true`)
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
      ##
      namespace: ''
      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ''
      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ''
      ## @param metrics.serviceMonitor.selector ServiceMonitor selector labels
      ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration
      ## e.g:
      ## selector:
      ##   prometheus: my-prometheus
      ##
      selector: {}
      ## @param metrics.serviceMonitor.relabelings Relabel configuration for the metrics
      ##
      relabelings: []
      ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []

  ## @section Kafka provisioning parameters

  ## Kafka provisioning
  ##
  provisioning:
    ## @param provisioning.enabled Enable kafka provisioning Job
    ##
    enabled: false
    ## @param provisioning.numPartitions Default number of partitions for topics when unspecified.
    numPartitions: 1
    ## @param provisioning.replicationFactor Default replication factor for topics when unspecified.
    replicationFactor: 1
    ## @param provisioning.schedulerName Name of the k8s scheduler (other than default) for kafka provisioning
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ''
    ## @param provisioning.podAnnotations Provisioning Pod annotations.
    ##
    podAnnotations: {}
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param provisioning.resources.limits The resources limits for the container
    ## @param provisioning.resources.requests The requested resources for the container
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 250m
      ##    memory: 1Gi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 250m
      ##    memory: 256Mi
      requests: {}
    ## @param provisioning.command Override provisioning container command
    ##
    command: []
    ## @param provisioning.args Override provisioning container arguments
    ##
    args: []
    ## @param provisioning.topics Kafka provisioning topics
    ## - name: topic-name
    ##   partitions: 1
    ##   replicationFactor: 1
    ##   ## https://kafka.apache.org/documentation/#topicconfigs
    ##   config:
    ##     max.message.bytes: 64000
    ##     flush.messages: 1
    ##
    topics: []

  ## @section Zookeeper chart parameters

  ## Zookeeper chart configuration
  ## https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/values.yaml
  ##
  zookeeper:
    ## @param zookeeper.enabled Switch to enable or disable the Zookeeper helm chart
    ##
    enabled: true
    persistence:
      ## @param persistence.existingClaim Provide an existing `PersistentVolumeClaim`
      ## If defined, PVC must be created manually before volume will be bound
      ## The value is evaluated as a template
      ##
      existingClaim: ''
      ## @param persistence.enabled Enable Zookeeper data persistence using PVC
      ##
      enabled: false
    auth:
      ## @param zookeeper.auth.enabled Enable Zookeeper auth
      ##
      enabled: false
      ## @param zookeeper.auth.clientUser User that will use Zookeeper clients to auth
      ##
      clientUser: ''
      ## @param zookeeper.auth.clientPassword Password that will use Zookeeper clients to auth
      ##
      clientPassword: ''
      ## @param zookeeper.auth.serverUsers Comma, semicolon or whitespace separated list of user to be created. Specify them as a string, for example: "user1,user2,admin"
      ##
      serverUsers: ''
      ## @param zookeeper.auth.serverPasswords Comma, semicolon or whitespace separated list of passwords to assign to users when created. Specify them as a string, for example: "pass4user1, pass4user2, pass4admin"
      ##
      serverPasswords: ''
  ## This value is only used when zookeeper.enabled is set to false
  ##
  externalZookeeper:
    ## @param externalZookeeper.servers Server or list of external Zookeeper servers to use
    ##
    servers: []

################################################################
## MySQL Backend Dependency
mysql:
  enabled: true
  ## Installation
  # https://bitnami.com/stack/mysql/helm
  # https://github.com/bitnami/charts/tree/master/bitnami/mysql
  # helm repo add bitnami https://charts.bitnami.com/bitnami
  # helm install db bitnami/mysql -f ./bitnami-mysql-charts.IGNORE.yaml --version 4.5.2

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets [array] Global Docker registry secret names as an array
  ## @param global.storageClass Global StorageClass for Persistent Volume(s)
  ##
  global:
    imageRegistry: ''
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    storageClass: ''

  ## @section Common parameters

  ## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
  ##
  nameOverride: ''
  ## @param fullnameOverride String to fully override common.names.fullname template
  ##
  fullnameOverride: 'mysql'
  ## @param clusterDomain Cluster domain
  ##
  clusterDomain: cluster.local
  ## @param commonAnnotations [object] Common annotations to add to all MySQL resources (sub-charts are not considered). Evaluated as a template
  ##
  commonAnnotations: {}
  ## @param commonLabels [object] Common labels to add to all MySQL resources (sub-charts are not considered). Evaluated as a template
  ##
  commonLabels: {}
  ## @param extraDeploy [array] Array with extra yaml to deploy with the chart. Evaluated as a template
  ##
  extraDeploy: []
  ## @param schedulerName Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ''

  ## Enable diagnostic mode in the deployment
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the deployment
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the deployment
    ##
    args:
      - infinity

  ## @section MySQL common parameters

  ## Bitnami MySQL image
  ## ref: https://hub.docker.com/r/bitnami/mysql/tags/
  ## @param image.registry MySQL image registry
  ## @param image.repository MySQL image repository
  ## @param image.tag MySQL image tag (immutable tags are recommended)
  ## @param image.pullPolicy MySQL image pull policy
  ## @param image.pullSecrets [array] Specify docker-registry secret names as an array
  ## @param image.debug Specify if debug logs should be enabled
  ##
  image:
    registry: docker.io
    repository: bitnami/mysql
    tag: 8.0.26-debian-10-r31
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Set to true if you would like to see extra information on logs
    ## It turns BASH and/or NAMI debugging in the image
    ##
    debug: false
  ## @param architecture MySQL architecture (`standalone` or `replication`)
  ##
  architecture: standalone
  ## MySQL Authentication parameters
  ##
  auth:
    ## @param auth.rootPassword Password for the `root` user. Ignored if existing secret is provided
    ## ref: https://github.com/bitnami/bitnami-docker-mysql#setting-the-root-password-on-first-run
    ##
    rootPassword: 'rootPassword'
    ## @param auth.database Name for a custom database to create
    ## ref: https://github.com/bitnami/bitnami-docker-mysql/blob/master/README.md#creating-a-database-on-first-run
    ##
    database: default
    ## @param auth.username Name for a custom user to create
    ## ref: https://github.com/bitnami/bitnami-docker-mysql/blob/master/README.md#creating-a-database-user-on-first-run
    ##
    username: 'user'
    ## @param auth.password Password for the new user. Ignored if existing secret is provided
    ##
    password: 'password'
    ## @param auth.replicationUser MySQL replication user
    ## ref: https://github.com/bitnami/bitnami-docker-mysql#setting-up-a-replication-cluster
    ##
    replicationUser: replicator
    ## @param auth.replicationPassword MySQL replication user password. Ignored if existing secret is provided
    ##
    replicationPassword: ''
    ## @param auth.existingSecret Use existing secret for password details. The secret has to contain the keys `mysql-root-password`, `mysql-replication-password` and `mysql-password`
    ## NOTE: When it's set the auth.rootPassword, auth.password, auth.replicationPassword are ignored.
    ##
    existingSecret: ''
    ## @param auth.forcePassword Force users to specify required passwords
    ##
    forcePassword: true
    ## @param auth.usePasswordFiles Mount credentials as files instead of using an environment variable
    ##
    usePasswordFiles: false
    ## @param auth.customPasswordFiles [object] Use custom password files when `auth.usePasswordFiles` is set to `true`. Define path for keys `root` and `user`, also define `replicator` if `architecture` is set to `replication`
    ## Example:
    ## customPasswordFiles:
    ##   root: /vault/secrets/mysql-root
    ##   user: /vault/secrets/mysql-user
    ##   replicator: /vault/secrets/mysql-replicator
    ##
    customPasswordFiles: {}
  ## @param initdbScripts [object] Dictionary of initdb scripts
  ## Specify dictionary of scripts to be run at first boot
  ## Example:
  ## initdbScripts:
  ##   my_init_script.sh: |
  ##      #!/bin/bash
  ##      echo "Do something."
  ##
  # initdbScripts: {}
  initdbScripts:
    # This script enables legacy authentication for MySQL v8. NodeJS MySQL Client currently does not support authentication plugins, reference: https://github.com/mysqljs/mysql/pull/2233
    enableLegacyAuth.sql: |-
      ALTER USER 'user'@'%' IDENTIFIED WITH mysql_native_password BY 'password';
  ## @param initdbScriptsConfigMap ConfigMap with the initdb scripts (Note: Overrides `initdbScripts`)
  ##
  initdbScriptsConfigMap: ''

  ## @section MySQL Primary parameters

  primary:
    ## @param primary.command [array] Override default container command on MySQL Primary container(s) (useful when using custom images)
    ##
    command: []
    ## @param primary.args [array] Override default container args on MySQL Primary container(s) (useful when using custom images)
    ##
    args: []
    ## @param primary.hostAliases [array] Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param primary.configuration [string] Configure MySQL Primary with a custom my.cnf file
    ## ref: https://mysql.com/kb/en/mysql/configuring-mysql-with-mycnf/#example-of-configuration-file
    ##
    configuration: |-
      [mysqld]
      default_authentication_plugin=mysql_native_password
      skip-name-resolve
      explicit_defaults_for_timestamp
      basedir=/opt/bitnami/mysql
      plugin_dir=/opt/bitnami/mysql/lib/plugin
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      datadir=/bitnami/mysql/data
      tmpdir=/opt/bitnami/mysql/tmp
      max_allowed_packet=16M
      bind-address=0.0.0.0
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
      log-error=/opt/bitnami/mysql/logs/mysqld.log
      character-set-server=UTF8
      collation-server=utf8_general_ci

      [client]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      default-character-set=UTF8
      plugin_dir=/opt/bitnami/mysql/lib/plugin

      [manager]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    ## @param primary.existingConfiguration Name of existing ConfigMap with MySQL Primary configuration.
    ## NOTE: When it's set the 'configuration' parameter is ignored
    ##
    existingConfiguration: ''
    ## @param primary.updateStrategy Update strategy type for the MySQL primary statefulset
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy: RollingUpdate
    ## @param primary.rollingUpdatePartition Partition update strategy for MySQL Primary statefulset
    ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
    ##
    rollingUpdatePartition: ''
    ## @param primary.podAnnotations [object] Additional pod annotations for MySQL primary pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param primary.podAffinityPreset MySQL primary pod affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ''
    ## @param primary.podAntiAffinityPreset MySQL primary pod anti-affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## MySQL Primary node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param primary.nodeAffinityPreset.type MySQL primary node affinity preset type. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ''
      ## @param primary.nodeAffinityPreset.key MySQL primary node label key to match Ignored if `primary.affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ''
      ## @param primary.nodeAffinityPreset.values [array] MySQL primary node label values to match. Ignored if `primary.affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param primary.affinity [object] Affinity for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param primary.nodeSelector [object] Node labels for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}
    ## @param primary.tolerations [array] Tolerations for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## MySQL primary Pod security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param primary.podSecurityContext.enabled Enable security context for MySQL primary pods
    ## @param primary.podSecurityContext.fsGroup Group ID for the mounted volumes' filesystem
    ##
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    ## MySQL primary container security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param primary.containerSecurityContext.enabled MySQL primary container securityContext
    ## @param primary.containerSecurityContext.runAsUser User ID for the MySQL primary container
    ##
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    ## MySQL primary container's resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param primary.resources.limits [object] The resources limits for MySQL primary containers
    ## @param primary.resources.requests [object] The requested resources for MySQL primary containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 250m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 250m
      ##    memory: 256Mi
      requests: {}
    ## Configure extra options for liveness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.livenessProbe.enabled Enable livenessProbe
    ## @param primary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param primary.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param primary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param primary.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param primary.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for readiness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.readinessProbe.enabled Enable readinessProbe
    ## @param primary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param primary.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param primary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param primary.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param primary.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for startupProbe probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.startupProbe.enabled Enable startupProbe
    ## @param primary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param primary.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param primary.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param primary.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param primary.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: true
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1
    ## @param primary.customLivenessProbe [object] Override default liveness probe for MySQL primary containers
    ##
    customLivenessProbe: {}
    ## @param primary.customReadinessProbe [object] Override default readiness probe for MySQL primary containers
    ##
    customReadinessProbe: {}
    ## @param primary.customStartupProbe [object] Override default startup probe for MySQL primary containers
    ##
    customStartupProbe: {}
    ## @param primary.extraFlags MySQL primary additional command line flags
    ## Can be used to specify command line flags, for example:
    ## E.g.
    ## extraFlags: "--max-connect-errors=1000 --max_connections=155"
    ##
    extraFlags: ''
    ## @param primary.extraEnvVars [array] Extra environment variables to be set on MySQL primary containers
    ## E.g.
    ## extraEnvVars:
    ##  - name: TZ
    ##    value: "Europe/Paris"
    ##
    extraEnvVars: []
    ## @param primary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for MySQL primary containers
    ##
    extraEnvVarsCM: ''
    ## @param primary.extraEnvVarsSecret Name of existing Secret containing extra env vars for MySQL primary containers
    ##
    extraEnvVarsSecret: ''
    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      ## @param primary.persistence.enabled Enable persistence on MySQL primary replicas using a `PersistentVolumeClaim`. If false, use emptyDir
      ##
      enabled: false
      ## @param primary.persistence.existingClaim Name of an existing `PersistentVolumeClaim` for MySQL primary replicas
      ## NOTE: When it's set the rest of persistence parameters are ignored
      ##
      existingClaim: ''
      ## @param primary.persistence.storageClass MySQL primary persistent volume storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ''
      ## @param primary.persistence.annotations [object] MySQL primary persistent volume claim annotations
      ##
      annotations: {}
      ## @param primary.persistence.accessModes MySQL primary persistent volume access Modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param primary.persistence.size MySQL primary persistent volume size
      ##
      size: 8Gi
      ## @param primary.persistence.selector [object] Selector to match an existing Persistent Volume
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @param primary.extraVolumes [array] Optionally specify extra list of additional volumes to the MySQL Primary pod(s)
    ##
    extraVolumes: []
    ## @param primary.extraVolumeMounts [array] Optionally specify extra list of additional volumeMounts for the MySQL Primary container(s)
    ##
    extraVolumeMounts: []
    ## @param primary.initContainers [array] Add additional init containers for the MySQL Primary pod(s)
    ##
    initContainers: []
    ## @param primary.sidecars [array] Add additional sidecar containers for the MySQL Primary pod(s)
    ##
    sidecars: []
    ## MySQL Primary Service parameters
    ##
    service:
      ## @param primary.service.type MySQL Primary K8s service type
      ##
      type: ClusterIP
      ## @param primary.service.port MySQL Primary K8s service port
      ##
      port: 3306
      ## @param primary.service.nodePort MySQL Primary K8s service node port
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePort: ''
      ## @param primary.service.clusterIP MySQL Primary K8s service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ''
      ## @param primary.service.loadBalancerIP MySQL Primary loadBalancerIP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ''
      ## @param primary.service.externalTrafficPolicy Enable client source IP preservation
      ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param primary.service.loadBalancerSourceRanges [array] Addresses that are allowed when MySQL Primary service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## E.g.
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param primary.service.annotations [object] Provide any additional annotations which may be required
      ##
      annotations: {}
    ## MySQL primary Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      ## @param primary.pdb.enabled Enable/disable a Pod Disruption Budget creation for MySQL primary pods
      ##
      enabled: false
      ## @param primary.pdb.minAvailable Minimum number/percentage of MySQL primary pods that should remain scheduled
      ##
      minAvailable: 1
      ## @param primary.pdb.maxUnavailable Maximum number/percentage of MySQL primary pods that may be made unavailable
      ##
      maxUnavailable: ''
    ## @param primary.podLabels [object] MySQL Primary pod label. If labels are same as commonLabels , this will take precedence
    ##
    podLabels: {}

  ## @section MySQL Secondary parameters

  secondary:
    ## @param secondary.replicaCount Number of MySQL secondary replicas
    ##
    replicaCount: 0
    ## @param secondary.hostAliases [array] Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param secondary.command [array] Override default container command on MySQL Secondary container(s) (useful when using custom images)
    ##
    command: []
    ## @param secondary.args [array] Override default container args on MySQL Secondary container(s) (useful when using custom images)
    ##
    args: []
    ## @param secondary.configuration [string] Configure MySQL Secondary with a custom my.cnf file
    ## ref: https://mysql.com/kb/en/mysql/configuring-mysql-with-mycnf/#example-of-configuration-file
    ##
    configuration: |-
      [mysqld]
      default_authentication_plugin=mysql_native_password
      skip-name-resolve
      explicit_defaults_for_timestamp
      basedir=/opt/bitnami/mysql
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      datadir=/bitnami/mysql/data
      tmpdir=/opt/bitnami/mysql/tmp
      max_allowed_packet=16M
      bind-address=0.0.0.0
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
      log-error=/opt/bitnami/mysql/logs/mysqld.log
      character-set-server=UTF8
      collation-server=utf8_general_ci

      [client]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      default-character-set=UTF8

      [manager]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    ## @param secondary.existingConfiguration Name of existing ConfigMap with MySQL Secondary configuration.
    ## NOTE: When it's set the 'configuration' parameter is ignored
    ##
    existingConfiguration: ''
    ## @param secondary.updateStrategy Update strategy type for the MySQL secondary statefulset
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy: RollingUpdate
    ## @param secondary.rollingUpdatePartition Partition update strategy for MySQL Secondary statefulset
    ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
    ##
    rollingUpdatePartition: ''
    ## @param secondary.podAnnotations [object] Additional pod annotations for MySQL secondary pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param secondary.podAffinityPreset MySQL secondary pod affinity preset. Ignored if `secondary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ''
    ## @param secondary.podAntiAffinityPreset MySQL secondary pod anti-affinity preset. Ignored if `secondary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ## Allowed values: soft, hard
    ##
    podAntiAffinityPreset: soft
    ## MySQL Secondary node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param secondary.nodeAffinityPreset.type MySQL secondary node affinity preset type. Ignored if `secondary.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ''
      ## @param secondary.nodeAffinityPreset.key MySQL secondary node label key to match Ignored if `secondary.affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ''
      ## @param secondary.nodeAffinityPreset.values [array] MySQL secondary node label values to match. Ignored if `secondary.affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param secondary.affinity [object] Affinity for MySQL secondary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param secondary.nodeSelector [object] Node labels for MySQL secondary pods assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}
    ## @param secondary.tolerations [array] Tolerations for MySQL secondary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## MySQL secondary Pod security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param secondary.podSecurityContext.enabled Enable security context for MySQL secondary pods
    ## @param secondary.podSecurityContext.fsGroup Group ID for the mounted volumes' filesystem
    ##
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    ## MySQL secondary container security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param secondary.containerSecurityContext.enabled MySQL secondary container securityContext
    ## @param secondary.containerSecurityContext.runAsUser User ID for the MySQL secondary container
    ##
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    ## MySQL secondary container's resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param secondary.resources.limits [object] The resources limits for MySQL secondary containers
    ## @param secondary.resources.requests [object] The requested resources for MySQL secondary containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 250m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 250m
      ##    memory: 256Mi
      requests: {}
    ## Configure extra options for liveness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param secondary.livenessProbe.enabled Enable livenessProbe
    ## @param secondary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param secondary.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param secondary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param secondary.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param secondary.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for readiness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param secondary.readinessProbe.enabled Enable readinessProbe
    ## @param secondary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param secondary.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param secondary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param secondary.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param secondary.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for startupProbe probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param secondary.startupProbe.enabled Enable startupProbe
    ## @param secondary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param secondary.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param secondary.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param secondary.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param secondary.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: true
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param secondary.customLivenessProbe [object] Override default liveness probe for MySQL secondary containers
    ##
    customLivenessProbe: {}
    ## @param secondary.customReadinessProbe [object] Override default readiness probe for MySQL secondary containers
    ##
    customReadinessProbe: {}
    ## @param secondary.customStartupProbe [object] Override default startup probe for MySQL secondary containers
    ##
    customStartupProbe: {}
    ## @param secondary.extraFlags MySQL secondary additional command line flags
    ## Can be used to specify command line flags, for example:
    ## E.g.
    ## extraFlags: "--max-connect-errors=1000 --max_connections=155"
    ##
    extraFlags: ''
    ## @param secondary.extraEnvVars [array] An array to add extra environment variables on MySQL secondary containers
    ## E.g.
    ## extraEnvVars:
    ##  - name: TZ
    ##    value: "Europe/Paris"
    ##
    extraEnvVars: []
    ## @param secondary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for MySQL secondary containers
    ##
    extraEnvVarsCM: ''
    ## @param secondary.extraEnvVarsSecret Name of existing Secret containing extra env vars for MySQL secondary containers
    ##
    extraEnvVarsSecret: ''
    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      ## @param secondary.persistence.enabled Enable persistence on MySQL secondary replicas using a `PersistentVolumeClaim`
      ##
      enabled: false
      ## @param secondary.persistence.storageClass MySQL secondary persistent volume storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ''
      ## @param secondary.persistence.annotations [object] MySQL secondary persistent volume claim annotations
      ##
      annotations: {}
      ## @param secondary.persistence.accessModes MySQL secondary persistent volume access Modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param secondary.persistence.size MySQL secondary persistent volume size
      ##
      size: 8Gi
      ## @param secondary.persistence.selector [object] Selector to match an existing Persistent Volume
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @param secondary.extraVolumes [array] Optionally specify extra list of additional volumes to the MySQL secondary pod(s)
    ##
    extraVolumes: []
    ## @param secondary.extraVolumeMounts [array] Optionally specify extra list of additional volumeMounts for the MySQL secondary container(s)
    ##
    extraVolumeMounts: []
    ## @param secondary.initContainers [array] Add additional init containers for the MySQL secondary pod(s)
    ##
    initContainers: []
    ## @param secondary.sidecars [array] Add additional sidecar containers for the MySQL secondary pod(s)
    ##
    sidecars: []
    ## MySQL Secondary Service parameters
    ##
    service:
      ## @param secondary.service.type MySQL secondary Kubernetes service type
      ##
      type: ClusterIP
      ## @param secondary.service.port MySQL secondary Kubernetes service port
      ##
      port: 3306
      ## @param secondary.service.nodePort MySQL secondary Kubernetes service node port
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePort: ''
      ## @param secondary.service.clusterIP MySQL secondary Kubernetes service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ''
      ## @param secondary.service.loadBalancerIP MySQL secondary loadBalancerIP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ''
      ## @param secondary.service.externalTrafficPolicy Enable client source IP preservation
      ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param secondary.service.loadBalancerSourceRanges [array] Addresses that are allowed when MySQL secondary service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## E.g.
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param secondary.service.annotations [object] Provide any additional annotations which may be required
      ##
      annotations: {}
    ## MySQL secondary Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      ## @param secondary.pdb.enabled Enable/disable a Pod Disruption Budget creation for MySQL secondary pods
      ##
      enabled: false
      ## @param secondary.pdb.minAvailable Minimum number/percentage of MySQL secondary pods that should remain scheduled
      ##
      minAvailable: 1
      ## @param secondary.pdb.maxUnavailable Maximum number/percentage of MySQL secondary pods that may be made unavailable
      ##
      maxUnavailable: ''
    ## @param secondary.podLabels [object] Additional pod labels for MySQL secondary pods
    ##
    podLabels: {}

  ## @section RBAC parameters

  ## MySQL pods ServiceAccount
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable the creation of a ServiceAccount for MySQL pods
    ##
    create: true
    ## @param serviceAccount.name Name of the created ServiceAccount
    ## If not set and create is true, a name is generated using the mysql.fullname template
    ##
    name: ''
    ## @param serviceAccount.annotations [object] Annotations for MySQL Service Account
    ##
    annotations: {}
  ## Role Based Access
  ## ref: https://kubernetes.io/docs/admin/authorization/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ##
    create: false

  ## @section Network Policy

  ## MySQL Nework Policy configuration
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: false
    ## @param networkPolicy.allowExternal The Policy model to apply.
    ## When set to false, only pods with the correct
    ## client label will have network access to the port MySQL is listening
    ## on. When true, MySQL will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.explicitNamespacesSelector [object] A Kubernetes LabelSelector to explicitly select namespaces from which ingress traffic could be allowed to MySQL
    ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace
    ## and that match other criteria, the ones that have the good label, can reach the DB.
    ## But sometimes, we want the DB to be accessible to clients from other namespaces, in this case, we can use this
    ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.
    ##
    ## Example:
    ## explicitNamespacesSelector:
    ##   matchLabels:
    ##     role: frontend
    ##   matchExpressions:
    ##    - {key: role, operator: In, values: [frontend]}
    ##
    explicitNamespacesSelector: {}

  ## @section Volume Permissions parameters

  ## Init containers parameters:
  ## volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`
    ##
    enabled: false
    ## @param volumePermissions.image.registry Init container volume-permissions image registry
    ## @param volumePermissions.image.repository Init container volume-permissions image repository
    ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets [array] Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r172
      pullPolicy: Always
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## @param volumePermissions.resources [object] Init container volume-permissions resources
    ##
    resources: {}

  ## @section Metrics parameters

  ## Mysqld Prometheus exporter parameters
  ##
  metrics:
    ## @param metrics.enabled Start a side-car prometheus exporter
    ##
    enabled: true
    ## @param metrics.image.registry Exporter image registry
    ## @param metrics.image.repository Exporter image repository
    ## @param metrics.image.tag Exporter image tag (immutable tags are recommended)
    ## @param metrics.image.pullPolicy Exporter image pull policy
    ## @param metrics.image.pullSecrets [array] Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/mysqld-exporter
      tag: 0.13.0-debian-10-r75
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## MySQL Prometheus exporter service parameters
    ## Mysqld Prometheus exporter liveness and readiness probes
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.service.type Kubernetes service type for MySQL Prometheus Exporter
    ## @param metrics.service.port MySQL Prometheus Exporter service port
    ## @param metrics.service.annotations [object] Prometheus exporter service annotations
    ##
    service:
      type: ClusterIP
      port: 9104
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
    ## @param metrics.extraArgs.primary [array] Extra args to be passed to mysqld_exporter on Primary pods
    ## @param metrics.extraArgs.secondary [array] Extra args to be passed to mysqld_exporter on Secondary pods
    ## ref: https://github.com/prometheus/mysqld_exporter/
    ## E.g.
    ## - --collect.auto_increment.columns
    ## - --collect.binlog_size
    ## - --collect.engine_innodb_status
    ## - --collect.engine_tokudb_status
    ## - --collect.global_status
    ## - --collect.global_variables
    ## - --collect.info_schema.clientstats
    ## - --collect.info_schema.innodb_metrics
    ## - --collect.info_schema.innodb_tablespaces
    ## - --collect.info_schema.innodb_cmp
    ## - --collect.info_schema.innodb_cmpmem
    ## - --collect.info_schema.processlist
    ## - --collect.info_schema.processlist.min_time
    ## - --collect.info_schema.query_response_time
    ## - --collect.info_schema.tables
    ## - --collect.info_schema.tables.databases
    ## - --collect.info_schema.tablestats
    ## - --collect.info_schema.userstats
    ## - --collect.perf_schema.eventsstatements
    ## - --collect.perf_schema.eventsstatements.digest_text_limit
    ## - --collect.perf_schema.eventsstatements.limit
    ## - --collect.perf_schema.eventsstatements.timelimit
    ## - --collect.perf_schema.eventswaits
    ## - --collect.perf_schema.file_events
    ## - --collect.perf_schema.file_instances
    ## - --collect.perf_schema.indexiowaits
    ## - --collect.perf_schema.tableiowaits
    ## - --collect.perf_schema.tablelocks
    ## - --collect.perf_schema.replication_group_member_stats
    ## - --collect.slave_status
    ## - --collect.slave_hosts
    ## - --collect.heartbeat
    ## - --collect.heartbeat.database
    ## - --collect.heartbeat.table
    ##
    extraArgs:
      primary: []
      secondary: []
    ## Mysqld Prometheus exporter resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param metrics.resources.limits [object] The resources limits for MySQL prometheus exporter containers
    ## @param metrics.resources.requests [object] The requested resources for MySQL prometheus exporter containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 100m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 100m
      ##    memory: 256Mi
      requests: {}
    ## Mysqld Prometheus exporter liveness probe
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.livenessProbe.enabled Enable livenessProbe
    ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 120
      periodSeconds: 10
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 3
    ## Mysqld Prometheus exporter readiness probe
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.readinessProbe.enabled Enable readinessProbe
    ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 3
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace Specify the namespace in which the serviceMonitor resource will be created
      ##
      namespace: ''
      ## @param metrics.serviceMonitor.interval Specify the interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
      ## e.g:
      ## scrapeTimeout: 30s
      ##
      scrapeTimeout: ''
      ## @param metrics.serviceMonitor.relabellings [array] Specify Metric Relabellings to add to the scrape endpoint
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels [object] Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      additionalLabels: {}

################################################################
## Kowl Backend Dependency
# NB: kowl turned off for the time being as the chart in the repo does not work with k8s v1.22
#     though the chart source appears to have been updated.
kowl:
  enabled: false
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 64Mi

  kowl:
    # See reference config: https://github.com/cloudhut/kowl/blob/master/docs/config/kowl.yaml)
    config:
      kafka:
        brokers:
          - kafka-headless:9092

  ingress:
    enabled: true
    className: ''
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

################################################################
## MongoDB Backend Dependency
reporting-events-db:
  enabled: true
  fullnameOverride: 'reporting-events-db'

  auth:
    rootUser: root
    rootPassword: 'rootPassword'
    username: 'user'
    password: 'password'
    database: 'default'

  resources:
    requests:
      cpu: 10m
      memory: 256Mi

  persistence:
    enabled: false
    accessModes:
      - ReadWriteOnce
    size: 8Gi
    annotations: {}

################################################################
## Ory Keto Backend Dependency
## If mysql is used as DB, run the following command from the keto container to setup the DB schemas
## keto migrate up -y --all-namespaces --config /etc/config/keto.yaml
##
keto:
  enabled: false
  ## Installation
  # https://k8s.ory.sh/helm/charts
  # helm repo add ory https://k8s.ory.sh/helm/charts
  # helm install my-keto ory/keto --version 0.19.5

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## @param global.storageClass Global StorageClass for Persistent Volume(s)
  ##
  global:
    imageRegistry: ''
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    storageClass: ''

  ## @section Common parameters

  ## @param nameOverride String to partially override kafka.fullname
  ##
  nameOverride: ''
  ## @param fullnameOverride String to fully override kafka.fullname
  ##
  fullnameOverride: 'keto'
  ## @param clusterDomain Default Kubernetes cluster domain
  ##
  clusterDomain: cluster.local
  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations: {}
  ## @param extraDeploy Array of extra objects to deploy with the release
  ##
  extraDeploy: []

  ## Enable diagnostic mode in the deployment
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the deployment
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the deployment
    ##
    args:
      - infinity

  ## @section Keto parameters

  ## Ory Keto image version
  ## ref: https://k8s.ory.sh/helm/charts
  ## @param image.registry Kafka image registry
  ## @param image.repository Keto image repository
  ## @param image.tag Kafka image tag (immutable tags are recommended)
  ## @param image.pullPolicy Kafka image pull policy
  ## @param image.pullSecrets Specify docker-registry secret names as an array
  ##
  # Default values for keto.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  # -- Number of replicas in deployment
  replicaCount: 1

  image:
    # -- Ory KETO image
    repository: oryd/keto
    # -- Default image pull policy
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    # -- Ory KETO version
    tag: 'v0.6.0-alpha.1-sqlite'

  imagePullSecrets: []

  serviceAccount:
    # -- Specifies whether a service account should be created
    create: true
    # -- Annotations to add to the service account
    annotations: {}
    # -- The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ''

  podAnnotations: {}

  podSecurityContext:
    {}
    # fsGroup: 2000

  # https://github.com/kubernetes/kubernetes/issues/57601
  automountServiceAccountToken: true

  # -- Default security context configuration
  securityContext:
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 100
    allowPrivilegeEscalation: false
    privileged: false

  job:
    annotations: {}

  ingress:
    read:
      enabled: true
      className: ''
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      hosts:
        - host: keto.local
          paths:
            - path: /read
              pathType: Prefix
      tls: []
      #  - secretName: keto-tls
      #    hosts:
      #      - keto.local
    write:
      enabled: false
      className: ''
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      hosts:
        - host: keto.local
          paths:
            - path: /write
              pathType: Prefix
      tls: []
      #  - secretName: keto-tls
      #    hosts:
      #      - keto.local

  service:
    read:
      enabled: true
      type: ClusterIP
      name: http-read
      port: 80
    write:
      enabled: true
      type: ClusterIP
      name: http-write
      port: 80

  secret:
    # -- Switch to false to prevent creating the secret
    enabled: true
    # ... and choose a different name for a secret you provide like this
    # nameOverride: "MyOtherName"
    secretAnnotations:
      # Create the secret before installation, and only then. This saves the secret from regenerating during an upgrade
      # pre-upgrade is needed to upgrade from 0.7.0 to newer. Can be deleted afterwards.
      helm.sh/hook-weight: '0'
      helm.sh/hook: 'pre-install, pre-upgrade'
      helm.sh/hook-delete-policy: 'before-hook-creation'
      helm.sh/resource-policy: 'keep'

  keto:
    # https://www.ory.sh/keto/docs/reference/configuration
    config:
      dsn: mysql://user:password@tcp(keto-db:3306)/keto?max_conns=20&max_idle_conns=4
      serve:
        read:
          port: 4466
        write:
          port: 4467
      namespaces:
        - id: 0
          name: role
        - id: 1
          name: permission
        - id: 2
          name: participant

    autoMigrate: false

  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  nodeSelector: {}

  extraEnv: []

  extraVolumes: []
  # - name: my-volume
  #   secret:
  #     secretName: my-secret

  extraVolumeMounts: []
  # - name: my-volume
  #   mountPath: /etc/secrets/my-secret
  #   readOnly: true

  # -- Configuration for tracing providers. Only datadog is currently supported through this block.
  # If you need to use a different tracing provider, please manually set the configuration values
  # via "keto.config" or via "extraEnv".
  tracing:
    datadog:
      enabled: false

      # Sets the datadog DD_ENV environment variable. This value indicates the environment where keto is running.
      # Default value: "none".
      # env: production

      # Sets the datadog DD_VERSION environment variable. This value indicates the version that keto is running.
      # Default value: .Chart.AppVersion (i.e. the tag used for the docker image).
      # version: X.Y.Z

      # Sets the datadog DD_SERVICE environment variable. This value indicates the name of the service running.
      # Default value: "ory/keto".
      # service: ory/keto

      # Sets the datadog DD_AGENT_HOST environment variable. This value indicates the host address of the datadog agent.
      # If set to true, this configuration will automatically set DD_AGENT_HOST to the field "status.hostIP" of the pod.
      # Default value: false.
      # useHostIP: true

  tolerations: []

  affinity: {}

  # -- Configure the probes for when the deployment is considered ready and ongoing health check
  deployment:
    livenessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 5
    readinessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 5

  # -- Watcher sidecar configuration
  watcher:
    enabled: false
    image: oryd/k8s-toolbox:0.0.2
    mountFile: ''
    # mountFile: /etc/secrets/my-secret/foo

  # -- PodDistributionBudget configuration
  pdb:
    enabled: false
    spec:
      minAvailable: 1

oathkeeper:
  enabled: false
  # -- If enabled, a demo deployment with exemplary access rules and JSON Web Key Secrets will be generated.
  demo: false

  fullnameOverride: 'oathkeeper'
  # -- Configures the Kubernetes service
  service:
    # -- Configures the Kubernetes service for the proxy port.
    proxy:
      # -- En-/disable the service
      enabled: true
      # -- The service type
      type: ClusterIP
      # -- The service port
      port: 4455
      # -- If you do want to specify annotations, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      labels: {}
      #      If you do want to specify additional labels, uncomment the following
      #      lines, adjust them as necessary, and remove the curly braces after 'labels:'.
      #      e.g.  app: oathkeeper

    # -- Configures the Kubernetes service for the api port.
    api:
      # -- En-/disable the service
      enabled: true
      # -- The service type
      type: ClusterIP
      # -- The service port
      port: 4456
      # -- If you do want to specify annotations, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      labels: {}
      #      If you do want to specify additional labels, uncomment the following
      #      lines, adjust them as necessary, and remove the curly braces after 'labels:'.
      #      e.g.  app: oathkeeper

  # -- Configure ingress
  ingress:
    # -- Configure ingress for the proxy port.
    proxy:
      # -- En-/Disable the proxy ingress.
      enabled: true
      className: ''
      annotations: {}
      #     kubernetes.io/ingress.class: nginx
      #     kubernetes.io/tls-acme: "true"
      hosts:
        - host: www-bof.local
          paths:
            - path: /proxy(/|$)(.*)
              pathType: ImplementationSpecific
              annotations:
                nginx.ingress.kubernetes.io/rewrite-target: /$2
    #    tls: []
    #        hosts:
    #          - www-bof.local
    #      - secretName: oathkeeper-proxy-example-tls

    api:
      # -- En-/Disable the api ingress.
      enabled: false
      className: ''
      annotations: {}
      #      If you do want to specify annotations, uncomment the following
      #      lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
      #      kubernetes.io/ingress.class: nginx
      #      kubernetes.io/tls-acme: "true"
      hosts:
        - host: api-oathkeeper.local
          paths:
            - path: /
              pathType: ImplementationSpecific
  #    tls: []
  #      hosts:
  #        - api-oathkeeper.local
  #      - secretName: oathkeeper-api-example-tls

  # -- Configure ORY Oathkeeper itself
  oathkeeper:
    # -- The ORY Oathkeeper configuration. For a full list of available settings, check:
    #   https://github.com/ory/oathkeeper/blob/master/docs/config.yaml
    config:
      log:
        level: info
      access_rules:
        matching_strategy: regexp
      authenticators:
        cookie_session:
          enabled: true
          config:
            # this should be the internal URL of the public Kratos service's whoami endpoint, which might look like the below
            check_session_url: http://kratos-public/sessions/whoami
            preserve_path: true
            # this means we automatically sweep up all the metadata kratos provides for use
            # in, for example, the JWT, if we ever have more
            extra_from: '@this'
            # kratos will be configured to put the subject from the IdP here
            subject_from: 'identity.traits.subject'
            only:
              - ory_kratos_session
        oauth2_introspection:
          enabled: true
          config:
            introspection_url: https://wso2-identity-server.local:9443/oauth2/introspect
            introspection_request_headers:
              # Configure the following with base64 encoded string contains admin:password of wso2 server
              authorization: 'Basic YWRtaW46YWRtaW4='
              # cache:
              #   # disabled to make debugging easier. enable for caching.
              #   enabled: false
              #   ttl: "60s"
      authorizers:
        remote_json:
          enabled: true
          config:
            # the check URL for Keto. This will be POST'd to. See https://www.ory.sh/keto/docs/reference/rest-api#operation/postCheck
            remote: http://keto-read/check
            payload: ''
      mutators:
        id_token:
          enabled: true
          config:
            # this should be the internal base URL for the API service, which will look something like the below
            issuer_url: http://oathkeeper-api:4456/
        header:
          # Set enabled to true if the authenticator should be enabled and false to disable the authenticator. Defaults to false.
          enabled: true
          config:
            headers:
              X-User: '{{ print .Subject }}'
      errors:
        fallback:
          - json
        handlers:
          json:
            # this gives API clients pretty error JSON
            enabled: true
            config:
              verbose: true
          redirect:
            enabled: true
            config:
              # set this to whatever the main URL is, it'll ensure that browser errors redirect there
              to: http://www-bof.local/
              when:
                - error:
                    - unauthorized
                    - forbidden
                  request:
                    header:
                      accept:
                        - text/html
      serve:
        proxy:
          port: 4455
        api:
          port: 4456
    # -- If set, uses the given JSON Web Key Set as the signing key for the ID Token Mutator.
    mutatorIdTokenJWKs: {}
    # -- If set, uses the given access rules.
    # accessRules: {}

    # -- If you enable maester, the following value should be set to "false" to avoid overwriting
    # the rules generated by the CDRs. Additionally, the value "accessRules" shouldn't be
    # used as it will have no effect once "managedAccessRules" is disabled.

    # This seems to be a bug in the charts - we MUST enable this,
    # otherwise the `oathkeeper-rules` configmap isn't created, and the pods fail to start
    # See: https://github.com/ory/k8s/blob/master/helm/charts/oathkeeper/templates/configmap-rules.yaml
    managedAccessRules: true

  secret:
    # -- set to true for this helm chart to manage the secret
    # -- set to false for this helm chart to NOT manage the secret
    # -- defaults to true
    manage: true

    # -- name of the secret to use. If empty, defaults to {{ include "oathkeeper.fullname" . }}
    name: 'oathkeeper-jwks'

    # -- default mount path for the kubernetes secret
    mountPath: /etc/secrets
    # -- default filename of JWKS (mounted as secret)
    filename: mutator.id_token.jwks.json

  deployment:
    resources: {}
    #  We usually recommend not to specify default resources and to leave this as a conscious
    #  choice for the user. This also increases chances charts run on environments with little
    #  resources, such as Minikube. If you do want to specify resources, uncomment the following
    #  lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #  memory: 128Mi
    securityContext:
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 1000
      allowPrivilegeEscalation: false
      privileged: false

    # https://github.com/kubernetes/kubernetes/issues/57601
    automountServiceAccountToken: false

    # -- Node labels for pod assignment.
    nodeSelector: {}
    # If you do want to specify node labels, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
    #   foo: bar

    extraEnv:
      # for whatever reason this environment variable only gets set if the JWKS is in the config even though the rest of the secret mounting
      # and such still happens
      - name: MUTATORS_ID_TOKEN_CONFIG_JWKS_URL
        value: file:///etc/secrets/mutator.id_token.jwks.json

    # -- Extra volumes you can attach to the pod.
    extraVolumes: []
    # - name: my-volume
    #   secret:
    #     secretName: my-secret

    # -- Extra volume mounts, allows mounting the extraVolumes to the container.
    extraVolumeMounts: []
    # - name: my-volume
    #   mountPath: /etc/secrets/my-secret
    #   readOnly: true

    # -- Configuration for tracing providers. Only datadog is currently supported through this block.
    # If you need to use a different tracing provider, please manually set the configuration values
    # via "oathkeeper.config" or via "deployment.extraEnv".
    tracing:
      datadog:
        enabled: false

        # -- Sets the datadog DD_ENV environment variable. This value indicates the environment where oathkeeper is running.
        # Default value: "none".
        # env: production

        # -- Sets the datadog DD_VERSION environment variable. This value indicates the version that oathkeeper is running.
        # Default value: .Values.image.tag (i.e. the tag used for the docker image).
        # version: X.Y.Z

        # -- Sets the datadog DD_SERVICE environment variable. This value indicates the name of the service running.
        # Default value: "ory/oathkeeper".
        # service: ory/oathkeeper

        # -- Sets the datadog DD_AGENT_HOST environment variable. This value indicates the host address of the datadog agent.
        # If set to true, this configuration will automatically set DD_AGENT_HOST to the field "status.hostIP" of the pod.
        # Default value: false.
        # useHostIP: true

    # -- Configure node tolerations.
    tolerations: []

    labels: {}
    #      If you do want to specify additional labels, uncomment the following
    #      lines, adjust them as necessary, and remove the curly braces after 'labels:'.
    #      e.g.  type: app

    annotations: {}
    #      If you do want to specify annotations, uncomment the following
    #      lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
    #      e.g.  sidecar.istio.io/rewriteAppHTTPProbers: "true"

  # -- Configure node affinity
  affinity: {}

  # -- Configures controller setup
  maester:
    enabled: true
    oathkeeperFullnameOverride: 'oathkeeper'

  # -- PodDistributionBudget configuration
  pdb:
    enabled: false
    spec:
      minAvailable: 1
  oathkeeper-maester:
    fullnameOverride: 'oathkeeper-maester'
    oathkeeperFullnameOverride: 'oathkeeper'
    deployment:
      envs:
        - name: authorizersAvailable
          value: allow,deny,noop,remote_json

## Ory Kratos
## If mysql is used as DB, run the following command from the kratos container to setup the DB schemas
## kratos migrate sql -e --yes
##
kratos:
  enabled: false
  # -- Number of replicas in deployment
  replicaCount: 1
  # -- Deployment update strategy
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 30%
      maxUnavailable: 0

  fullnameOverride: 'kratos'

  service:
    admin:
      enabled: true
      type: ClusterIP
      port: 80
      # -- If you do want to specify annotations, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    public:
      enabled: true
      type: ClusterIP
      port: 80
      # -- If you do want to specify annotations, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"

  secret:
    # -- switch to false to prevent creating the secret
    enabled: true
    # ... and choose a different name for a secret you provide like this
    # nameOverride: "MyOtherName"
    secretAnnotations:
      # Create the secret before installation, and only then. This saves the secret from regenerating during an upgrade
      # pre-upgrade is needed to upgrade from 0.7.0 to newer. Can be deleted afterwards.
      helm.sh/hook-weight: '0'
      helm.sh/hook: 'pre-install, pre-upgrade'
      helm.sh/hook-delete-policy: 'before-hook-creation'
      helm.sh/resource-policy: 'keep'

  ingress:
    admin:
      enabled: false
      className: ''
      annotations:
        {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      hosts:
        - host: kratos-admin.local
          paths:
            - /
      tls: []
      #  - secretName: kratos-admin-tls
      #    hosts:
      #      - kratos-admin.local
    public:
      ## Disabling the following required ingress for fixing an issue with ingress in K8S version 1.22+
      ## For K8S versions less than 1.22, we can just enable this and use the config
      ## We can't use this config in K8S 1.22+ unless we upgrade the kratos chart version
      enabled: false
      className: ''
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      hosts:
        - host: www-bof.local
          paths:
            - /kratos(/|$)(.*)
      tls: []
      #  - secretName: kratos-public-tls
      #    hosts:
      #      - www-bof.local

  kratos:
    log:
      ## Leak Sensitive Log Values ##
      #
      # If set will leak sensitive values (e.g. emails) in the logs.
      #
      # Set this value using environment variables on
      # - Linux/macOS:
      #    $ export LOG_LEAK_SENSITIVE_VALUES=<value>
      # - Windows Command Line (CMD):
      #    > set LOG_LEAK_SENSITIVE_VALUES=<value>
      #
      leak_sensitive_values: false
      ## format ##
      #
      # The log format can either be text or JSON.
      #
      # One of:
      # - json
      # - text
      #
      # Set this value using environment variables on
      # - Linux/macOS:
      #    $ export LOG_FORMAT=<value>
      # - Windows Command Line (CMD):
      #    > set LOG_FORMAT=<value>
      #
      format: text
      ## level ##
      #
      # Debug enables stack traces on errors. Can also be set using environment variable LOG_LEVEL.
      #
      # Default value: info
      #
      # One of:
      # - trace
      # - debug
      # - info
      # - warning
      # - error
      # - fatal
      # - panic
      level: info
    development: false
    # -- Enable the initialization job. Required to work with a DB
    autoMigrate: false

    # -- You can add multiple identity schemas here
    identitySchemas:
      'identity.default.schema.json': |
        {
          "$id": "https://mojaloop.io/kratos-schema/identity.schema.json",
          "$schema": "http://json-schema.org/draft-07/schema#",
          "title": "Person",
          "type": "object",
          "properties": {
            "traits": {
              "type": "object",
              "properties": {
                "email": {
                  "title": "E-Mail",
                  "type": "string",
                  "format": "email"
                },
                "subject": {
                  "title": "Subject",
                  "type": "string"
                },
                "name": {
                  "title": "Name",
                  "type": "string"
                }
              }
            }
          }
        }

    config:
      # dsn: memory
      dsn: mysql://user:password@tcp(kratos-db:3306)/kratos?max_conns=20&max_idle_conns=4
      # There seems to be some issue with the kratos charts where this
      # is still required in the deployment even if disabled.
      # https://github.com/ory/k8s/blob/master/helm/charts/kratos/templates/statefulset-mail.yaml#L61
      courier:
        smtp:
          connection_uri: smtps://test:test@mailslurper:1025/?skip_ssl_verify=true&legacy_ssl=true
      serve:
        public:
          base_url: http://www-bof.local/kratos/
          port: 4433
          cors:
            enabled: true
        admin:
          port: 4434

      selfservice:
        default_browser_return_url: http://www-bof.local/
        whitelisted_return_urls:
          - http://www-bof.local/

        methods:
          oidc:
            enabled: true
            config:
              providers:
                - id: idp
                  provider: generic
                  # TODO both the client_id and client_secret need to be set appropriately to the client supporting authorization code grants with openid
                  # TODO these can alternatively be set via environment variable from a k8s secret
                  client_id: provide-client-id-here
                  client_secret: provide-client-secret-here
                  # mapper_url: file:///etc/config2/oidc.jsonnet
                  mapper_url: base64://bG9jYWwgY2xhaW1zID0gc3RkLmV4dFZhcignY2xhaW1zJyk7Cgp7CiAgaWRlbnRpdHk6IHsKICAgIHRyYWl0czogewogICAgICBlbWFpbDogY2xhaW1zLmVtYWlsLAogICAgICBuYW1lOiBjbGFpbXMubmFtZSwKICAgICAgc3ViamVjdDogY2xhaW1zLnN1YgogICAgfSwKICB9LAp9
                  # issuer_url is the OpenID Connect Server URL. You can leave this empty if `provider` is not set to `generic`.
                  # If set, neither `auth_url` nor `token_url` are required.
                  issuer_url: https://wso2-identity-server.local:9443/oauth2/oidcdiscovery

                  # auth_url is the authorize url, typically something like: https://example.org/oauth2/auth
                  # Should only be used when the OAuth2 / OpenID Connect server is not supporting OpenID Connect Discovery and when
                  # `provider` is set to `generic`.
                  # auth_url: http://openid-connect-provider/oauth2/auth

                  # token_url is the token url, typically something like: https://example.org/oauth2/token
                  # Should only be used when the OAuth2 / OpenID Connect server is not supporting OpenID Connect Discovery and when
                  # `provider` is set to `generic`.
                  # token_url: http://openid-connect-provider/oauth2/token
                  scope:
                    # # TODO adjust requested scope based on IdP (WSO2) documentation
                    - openid
        flows:
          # error:
          #   ui_url: http://www-bof.local/selfui/error

          # settings:
          #   ui_url: http://www-bof.local/selfui/settings
          #   privileged_session_max_age: 15m

          # recovery:
          #   enabled: true
          #   ui_url: http://www-bof.local/selfui/recovery

          # verification:
          #   enabled: true
          #   ui_url: http://www-bof.local/selfui/verify
          #   after:
          #     default_browser_return_url: http://www-bof.local/selfui/

          login:
            ui_url: http://www-bof.local/selfui/auth/login
            lifespan: 10m

          logout:
            after:
              default_browser_return_url: https://wso2-identity-server.local:9443/oidc/logout

          registration:
            lifespan: 10m
            ui_url: http://www-bof.local/selfui/auth/
            after:
              oidc:
                hooks:
                  - hook: session
      secrets:
        cookie:
          - PLEASE-CHANGE-ME-I-AM-VERY-INSECURE
      hashers:
        argon2:
          parallelism: 1
          ## This one is changed in the new kratos version
          ## This change is because the kratos docker image version is overridden to older version. See the comments at image parameter above.
          # memory: "128MB"
          memory: 120000
          iterations: 3
          salt_length: 16
          key_length: 32
      identity:
        default_schema_url: file:///etc/config/identity.default.schema.json

  deployment:
    # -- Configure the probes for when the deployment is considered ready and ongoing health check
    livenessProbe:
      httpGet:
        path: /health/alive
        port: http-admin
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 5
    readinessProbe:
      httpGet:
        path: /health/ready
        port: http-admin
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 5

    extraEnv: []
    # -- If you want to mount external volume
    # For example, mount a secret containing Certificate root CA to verify database
    # TLS connection.
    extraVolumes: []
    # - name: my-volume
    #   secret:
    #     secretName: my-secret
    extraVolumeMounts: []
    # - name: my-volume
    #   mountPath: /etc/secrets/my-secret
    #   readOnly: true

    # extraVolumes:
    #   - name: postgresql-tls
    #     secret:
    #       secretName: postgresql-root-ca
    # extraVolumeMounts:
    #   - name: postgresql-tls
    #     mountPath: "/etc/postgresql-tls"
    #     readOnly: true

    # -- If you want to add extra init containers. These are processed before the migration init container.
    # extraInitContainers: {}
    # extraInitContainers: |
    #  - name: ...
    #    image: ...
    extraInitContainers: |-
      - name: wait-for-mysql
        image: mysql
        imagePullPolicy: IfNotPresent
        command:
          - sh
          - -c
          - until mysql -h ${DB_HOST} -P ${DB_PORT} -u ${DB_USER} --password=${DB_PASSWORD}  ${DB_DATABASE} -e 'select version()' ;
            do
              echo --------------------;
              echo Waiting for MySQL...;
              sleep 2;
            done;
            echo ====================;
            echo MySQL ok!;
        env:
        - name: DB_HOST
          value: 'kratos-db'
        - name: DB_PORT
          value: '3306'
        - name: DB_USER
          value: 'user'
        - name: DB_PASSWORD
          value: 'password'
        - name: DB_DATABASE
          value: 'kratos'

    # -- Configuration for tracing providers. Only datadog is currently supported through this block.
    # If you need to use a different tracing provider, please manually set the configuration values
    # via "kratos.config" or via "deployment.extraEnv".
    tracing:
      datadog:
        enabled: false

        # Sets the datadog DD_ENV environment variable. This value indicates the environment where kratos is running.
        # Default value: "none".
        # env: production

        # Sets the datadog DD_VERSION environment variable. This value indicates the version that kratos is running.
        # Default value: .Values.image.tag (i.e. the tag used for the docker image).
        # version: X.Y.Z

        # Sets the datadog DD_SERVICE environment variable. This value indicates the name of the service running.
        # Default value: "ory/kratos".
        # service: ory/kratos

        # Sets the datadog DD_AGENT_HOST environment variable. This value indicates the host address of the datadog agent.
        # If set to true, this configuration will automatically set DD_AGENT_HOST to the field "status.hostIP" of the pod.
        # Default value: false.
        # useHostIP: true

    resources: {}
    #  We usually recommend not to specify default resources and to leave this as a conscious
    #  choice for the user. This also increases chances charts run on environments with little
    #  resources, such as Minikube. If you do want to specify resources, uncomment the following
    #  lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #  memory: 128Mi

    # -- Node labels for pod assignment.
    nodeSelector: {}
    # If you do want to specify node labels, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
    #   foo: bar

    # -- Configure node tolerations.
    tolerations: []

    labels: {}
    #      If you do want to specify additional labels, uncomment the following
    #      lines, adjust them as necessary, and remove the curly braces after 'labels:'.
    #      e.g.  type: app

    annotations: {}
    #      If you do want to specify annotations, uncomment the following
    #      lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
    #      e.g.  sidecar.istio.io/rewriteAppHTTPProbers: "true"

    # -- The secret specified here will be used to load environment variables with envFrom.
    # This allows arbitrary environment variables to be provided to the application which is useful for
    # sensitive values which should not be in a configMap.
    # This secret is not created by the helm chart and must already exist in the namespace.
    # https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables
    # environmentSecretsName:

    # https://github.com/kubernetes/kubernetes/issues/57601
    automountServiceAccountToken: true

  securityContext:
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 100
    allowPrivilegeEscalation: false
    privileged: false

  # -- Horizontal pod autoscaling configuration
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Values for initialization job
  job:
    annotations: {}
    ttlSecondsAfterFinished: 60

  statefulset:
    log:
      format: json
      level: trace

  # -- Configure node affinity
  affinity: {}
  # -- Node labels for pod assignment.
  nodeSelector: {}
  # -- If you do want to specify node labels, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'annotations:'.
  #   foo: bar
  # Configure node tolerations.
  tolerations: []

  watcher:
    enabled: false
    image: oryd/k8s-toolbox:0.0.2
    mountFile: ''
    # mountFile: /etc/secrets/my-secret/foo

  # -- PodDistributionBudget configuration
  pdb:
    enabled: false
    spec:
      minAvailable: 1

################################################################
## MySQL DB for Kratos
kratos-db:
  enabled: false
  fullnameOverride: 'kratos-db'
  image:
    registry: docker.io
    repository: bitnami/mysql
    tag: 8.0.26-debian-10-r31
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Set to true if you would like to see extra information on logs
    ## It turns BASH and/or NAMI debugging in the image
    ##
    debug: false
  ## @param architecture MySQL architecture (`standalone` or `replication`)
  ##
  architecture: standalone
  ## MySQL Authentication parameters
  ##
  auth:
    ## @param auth.rootPassword Password for the `root` user. Ignored if existing secret is provided
    ## ref: https://github.com/bitnami/bitnami-docker-mysql#setting-the-root-password-on-first-run
    ##
    rootPassword: 'rootPassword'
    ## @param auth.database Name for a custom database to create
    ## ref: https://github.com/bitnami/bitnami-docker-mysql/blob/master/README.md#creating-a-database-on-first-run
    ##
    database: kratos
    ## @param auth.username Name for a custom user to create
    ## ref: https://github.com/bitnami/bitnami-docker-mysql/blob/master/README.md#creating-a-database-user-on-first-run
    ##
    username: 'user'
    ## @param auth.password Password for the new user. Ignored if existing secret is provided
    ##
    password: 'password'
    ## @param auth.replicationUser MySQL replication user
    ## ref: https://github.com/bitnami/bitnami-docker-mysql#setting-up-a-replication-cluster
    ##
    replicationUser: replicator
    ## @param auth.replicationPassword MySQL replication user password. Ignored if existing secret is provided
    ##
    replicationPassword: ''
    ## @param auth.existingSecret Use existing secret for password details. The secret has to contain the keys `mysql-root-password`, `mysql-replication-password` and `mysql-password`
    ## NOTE: When it's set the auth.rootPassword, auth.password, auth.replicationPassword are ignored.
    ##
    existingSecret: ''
    ## @param auth.forcePassword Force users to specify required passwords
    ##
    forcePassword: true
    ## @param auth.usePasswordFiles Mount credentials as files instead of using an environment variable
    ##
    usePasswordFiles: false
    ## @param auth.customPasswordFiles [object] Use custom password files when `auth.usePasswordFiles` is set to `true`. Define path for keys `root` and `user`, also define `replicator` if `architecture` is set to `replication`
    ## Example:
    ## customPasswordFiles:
    ##   root: /vault/secrets/mysql-root
    ##   user: /vault/secrets/mysql-user
    ##   replicator: /vault/secrets/mysql-replicator
    ##
    customPasswordFiles: {}
  ## @param initdbScripts [object] Dictionary of initdb scripts
  ## Specify dictionary of scripts to be run at first boot
  ## Example:
  ## initdbScripts:
  ##   my_init_script.sh: |
  ##      #!/bin/bash
  ##      echo "Do something."
  ##
  # initdbScripts: {}
  initdbScripts:
    # This script enables legacy authentication for MySQL v8. NodeJS MySQL Client currently does not support authentication plugins, reference: https://github.com/mysqljs/mysql/pull/2233
    enableLegacyAuth.sql: |-
      ALTER USER 'user'@'%' IDENTIFIED WITH mysql_native_password BY 'password';
  ## @param initdbScriptsConfigMap ConfigMap with the initdb scripts (Note: Overrides `initdbScripts`)
  ##
  initdbScriptsConfigMap: ''

  ## @section MySQL Primary parameters

  primary:
    ## @param primary.command [array] Override default container command on MySQL Primary container(s) (useful when using custom images)
    ##
    command: []
    ## @param primary.args [array] Override default container args on MySQL Primary container(s) (useful when using custom images)
    ##
    args: []
    ## @param primary.hostAliases [array] Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param primary.configuration [string] Configure MySQL Primary with a custom my.cnf file
    ## ref: https://mysql.com/kb/en/mysql/configuring-mysql-with-mycnf/#example-of-configuration-file
    ##
    configuration: |-
      [mysqld]
      default_authentication_plugin=mysql_native_password
      skip-name-resolve
      explicit_defaults_for_timestamp
      basedir=/opt/bitnami/mysql
      plugin_dir=/opt/bitnami/mysql/lib/plugin
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      datadir=/bitnami/mysql/data
      tmpdir=/opt/bitnami/mysql/tmp
      max_allowed_packet=16M
      bind-address=0.0.0.0
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
      log-error=/opt/bitnami/mysql/logs/mysqld.log
      character-set-server=UTF8
      collation-server=utf8_general_ci

      [client]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      default-character-set=UTF8
      plugin_dir=/opt/bitnami/mysql/lib/plugin

      [manager]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    ## @param primary.existingConfiguration Name of existing ConfigMap with MySQL Primary configuration.
    ## NOTE: When it's set the 'configuration' parameter is ignored
    ##
    existingConfiguration: ''
    ## @param primary.updateStrategy Update strategy type for the MySQL primary statefulset
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy: RollingUpdate
    ## @param primary.rollingUpdatePartition Partition update strategy for MySQL Primary statefulset
    ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
    ##
    rollingUpdatePartition: ''
    ## @param primary.podAnnotations [object] Additional pod annotations for MySQL primary pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param primary.podAffinityPreset MySQL primary pod affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ''
    ## @param primary.podAntiAffinityPreset MySQL primary pod anti-affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## MySQL Primary node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param primary.nodeAffinityPreset.type MySQL primary node affinity preset type. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ''
      ## @param primary.nodeAffinityPreset.key MySQL primary node label key to match Ignored if `primary.affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ''
      ## @param primary.nodeAffinityPreset.values [array] MySQL primary node label values to match. Ignored if `primary.affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param primary.affinity [object] Affinity for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param primary.nodeSelector [object] Node labels for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}
    ## @param primary.tolerations [array] Tolerations for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## MySQL primary Pod security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param primary.podSecurityContext.enabled Enable security context for MySQL primary pods
    ## @param primary.podSecurityContext.fsGroup Group ID for the mounted volumes' filesystem
    ##
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    ## MySQL primary container security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param primary.containerSecurityContext.enabled MySQL primary container securityContext
    ## @param primary.containerSecurityContext.runAsUser User ID for the MySQL primary container
    ##
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    ## MySQL primary container's resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param primary.resources.limits [object] The resources limits for MySQL primary containers
    ## @param primary.resources.requests [object] The requested resources for MySQL primary containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 250m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 250m
      ##    memory: 256Mi
      requests: {}
    ## Configure extra options for liveness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.livenessProbe.enabled Enable livenessProbe
    ## @param primary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param primary.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param primary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param primary.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param primary.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for readiness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.readinessProbe.enabled Enable readinessProbe
    ## @param primary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param primary.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param primary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param primary.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param primary.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for startupProbe probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.startupProbe.enabled Enable startupProbe
    ## @param primary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param primary.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param primary.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param primary.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param primary.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: true
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1
    ## @param primary.customLivenessProbe [object] Override default liveness probe for MySQL primary containers
    ##
    customLivenessProbe: {}
    ## @param primary.customReadinessProbe [object] Override default readiness probe for MySQL primary containers
    ##
    customReadinessProbe: {}
    ## @param primary.customStartupProbe [object] Override default startup probe for MySQL primary containers
    ##
    customStartupProbe: {}
    ## @param primary.extraFlags MySQL primary additional command line flags
    ## Can be used to specify command line flags, for example:
    ## E.g.
    ## extraFlags: "--max-connect-errors=1000 --max_connections=155"
    ##
    extraFlags: ''
    ## @param primary.extraEnvVars [array] Extra environment variables to be set on MySQL primary containers
    ## E.g.
    ## extraEnvVars:
    ##  - name: TZ
    ##    value: "Europe/Paris"
    ##
    extraEnvVars: []
    ## @param primary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for MySQL primary containers
    ##
    extraEnvVarsCM: ''
    ## @param primary.extraEnvVarsSecret Name of existing Secret containing extra env vars for MySQL primary containers
    ##
    extraEnvVarsSecret: ''
    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      ## @param primary.persistence.enabled Enable persistence on MySQL primary replicas using a `PersistentVolumeClaim`. If false, use emptyDir
      ##
      enabled: false
      ## @param primary.persistence.existingClaim Name of an existing `PersistentVolumeClaim` for MySQL primary replicas
      ## NOTE: When it's set the rest of persistence parameters are ignored
      ##
      existingClaim: ''
      ## @param primary.persistence.storageClass MySQL primary persistent volume storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ''
      ## @param primary.persistence.annotations [object] MySQL primary persistent volume claim annotations
      ##
      annotations: {}
      ## @param primary.persistence.accessModes MySQL primary persistent volume access Modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param primary.persistence.size MySQL primary persistent volume size
      ##
      size: 8Gi
      ## @param primary.persistence.selector [object] Selector to match an existing Persistent Volume
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @param primary.extraVolumes [array] Optionally specify extra list of additional volumes to the MySQL Primary pod(s)
    ##
    extraVolumes: []
    ## @param primary.extraVolumeMounts [array] Optionally specify extra list of additional volumeMounts for the MySQL Primary container(s)
    ##
    extraVolumeMounts: []
    ## @param primary.initContainers [array] Add additional init containers for the MySQL Primary pod(s)
    ##
    initContainers: []
    ## @param primary.sidecars [array] Add additional sidecar containers for the MySQL Primary pod(s)
    ##
    sidecars: []
    ## MySQL Primary Service parameters
    ##
    service:
      ## @param primary.service.type MySQL Primary K8s service type
      ##
      type: ClusterIP
      ## @param primary.service.port MySQL Primary K8s service port
      ##
      port: 3306
      ## @param primary.service.nodePort MySQL Primary K8s service node port
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePort: ''
      ## @param primary.service.clusterIP MySQL Primary K8s service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ''
      ## @param primary.service.loadBalancerIP MySQL Primary loadBalancerIP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ''
      ## @param primary.service.externalTrafficPolicy Enable client source IP preservation
      ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param primary.service.loadBalancerSourceRanges [array] Addresses that are allowed when MySQL Primary service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## E.g.
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param primary.service.annotations [object] Provide any additional annotations which may be required
      ##
      annotations: {}
    ## MySQL primary Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      ## @param primary.pdb.enabled Enable/disable a Pod Disruption Budget creation for MySQL primary pods
      ##
      enabled: false
      ## @param primary.pdb.minAvailable Minimum number/percentage of MySQL primary pods that should remain scheduled
      ##
      minAvailable: 1
      ## @param primary.pdb.maxUnavailable Maximum number/percentage of MySQL primary pods that may be made unavailable
      ##
      maxUnavailable: ''
    ## @param primary.podLabels [object] MySQL Primary pod label. If labels are same as commonLabels , this will take precedence
    ##
    podLabels: {}

  ## @section RBAC parameters

  ## MySQL pods ServiceAccount
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable the creation of a ServiceAccount for MySQL pods
    ##
    create: true
    ## @param serviceAccount.name Name of the created ServiceAccount
    ## If not set and create is true, a name is generated using the mysql.fullname template
    ##
    name: ''
    ## @param serviceAccount.annotations [object] Annotations for MySQL Service Account
    ##
    annotations: {}
  ## Role Based Access
  ## ref: https://kubernetes.io/docs/admin/authorization/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ##
    create: false

  ## @section Network Policy

  ## MySQL Nework Policy configuration
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: false
    ## @param networkPolicy.allowExternal The Policy model to apply.
    ## When set to false, only pods with the correct
    ## client label will have network access to the port MySQL is listening
    ## on. When true, MySQL will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.explicitNamespacesSelector [object] A Kubernetes LabelSelector to explicitly select namespaces from which ingress traffic could be allowed to MySQL
    ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace
    ## and that match other criteria, the ones that have the good label, can reach the DB.
    ## But sometimes, we want the DB to be accessible to clients from other namespaces, in this case, we can use this
    ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.
    ##
    ## Example:
    ## explicitNamespacesSelector:
    ##   matchLabels:
    ##     role: frontend
    ##   matchExpressions:
    ##    - {key: role, operator: In, values: [frontend]}
    ##
    explicitNamespacesSelector: {}

  ## @section Volume Permissions parameters

  ## Init containers parameters:
  ## volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`
    ##
    enabled: false
    ## @param volumePermissions.image.registry Init container volume-permissions image registry
    ## @param volumePermissions.image.repository Init container volume-permissions image repository
    ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets [array] Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r172
      pullPolicy: Always
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## @param volumePermissions.resources [object] Init container volume-permissions resources
    ##
    resources: {}

  ## @section Metrics parameters

  ## Mysqld Prometheus exporter parameters
  ##
  metrics:
    ## @param metrics.enabled Start a side-car prometheus exporter
    ##
    enabled: false
    ## @param metrics.image.registry Exporter image registry
    ## @param metrics.image.repository Exporter image repository
    ## @param metrics.image.tag Exporter image tag (immutable tags are recommended)
    ## @param metrics.image.pullPolicy Exporter image pull policy
    ## @param metrics.image.pullSecrets [array] Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/mysqld-exporter
      tag: 0.13.0-debian-10-r75
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## MySQL Prometheus exporter service parameters
    ## Mysqld Prometheus exporter liveness and readiness probes
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.service.type Kubernetes service type for MySQL Prometheus Exporter
    ## @param metrics.service.port MySQL Prometheus Exporter service port
    ## @param metrics.service.annotations [object] Prometheus exporter service annotations
    ##
    service:
      type: ClusterIP
      port: 9104
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
    ## @param metrics.extraArgs.primary [array] Extra args to be passed to mysqld_exporter on Primary pods
    ## @param metrics.extraArgs.secondary [array] Extra args to be passed to mysqld_exporter on Secondary pods
    ## ref: https://github.com/prometheus/mysqld_exporter/
    ## E.g.
    ## - --collect.auto_increment.columns
    ## - --collect.binlog_size
    ## - --collect.engine_innodb_status
    ## - --collect.engine_tokudb_status
    ## - --collect.global_status
    ## - --collect.global_variables
    ## - --collect.info_schema.clientstats
    ## - --collect.info_schema.innodb_metrics
    ## - --collect.info_schema.innodb_tablespaces
    ## - --collect.info_schema.innodb_cmp
    ## - --collect.info_schema.innodb_cmpmem
    ## - --collect.info_schema.processlist
    ## - --collect.info_schema.processlist.min_time
    ## - --collect.info_schema.query_response_time
    ## - --collect.info_schema.tables
    ## - --collect.info_schema.tables.databases
    ## - --collect.info_schema.tablestats
    ## - --collect.info_schema.userstats
    ## - --collect.perf_schema.eventsstatements
    ## - --collect.perf_schema.eventsstatements.digest_text_limit
    ## - --collect.perf_schema.eventsstatements.limit
    ## - --collect.perf_schema.eventsstatements.timelimit
    ## - --collect.perf_schema.eventswaits
    ## - --collect.perf_schema.file_events
    ## - --collect.perf_schema.file_instances
    ## - --collect.perf_schema.indexiowaits
    ## - --collect.perf_schema.tableiowaits
    ## - --collect.perf_schema.tablelocks
    ## - --collect.perf_schema.replication_group_member_stats
    ## - --collect.slave_status
    ## - --collect.slave_hosts
    ## - --collect.heartbeat
    ## - --collect.heartbeat.database
    ## - --collect.heartbeat.table
    ##
    extraArgs:
      primary: []
      secondary: []
    ## Mysqld Prometheus exporter resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param metrics.resources.limits [object] The resources limits for MySQL prometheus exporter containers
    ## @param metrics.resources.requests [object] The requested resources for MySQL prometheus exporter containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 100m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 100m
      ##    memory: 256Mi
      requests: {}
    ## Mysqld Prometheus exporter liveness probe
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.livenessProbe.enabled Enable livenessProbe
    ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 120
      periodSeconds: 10
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 3
    ## Mysqld Prometheus exporter readiness probe
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.readinessProbe.enabled Enable readinessProbe
    ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 3
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator
      ##
      enabled: true
      ## @param metrics.serviceMonitor.namespace Specify the namespace in which the serviceMonitor resource will be created
      ##
      namespace: ''
      ## @param metrics.serviceMonitor.interval Specify the interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
      ## e.g:
      ## scrapeTimeout: 30s
      ##
      scrapeTimeout: ''
      ## @param metrics.serviceMonitor.relabellings [array] Specify Metric Relabellings to add to the scrape endpoint
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels [object] Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      additionalLabels: {}

################################################################
## MySQL DB for Keto
keto-db:
  enabled: false
  fullnameOverride: 'keto-db'
  image:
    registry: docker.io
    repository: bitnami/mysql
    tag: 8.0.26-debian-10-r31
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Set to true if you would like to see extra information on logs
    ## It turns BASH and/or NAMI debugging in the image
    ##
    debug: false
  ## @param architecture MySQL architecture (`standalone` or `replication`)
  ##
  architecture: standalone
  ## MySQL Authentication parameters
  ##
  auth:
    ## @param auth.rootPassword Password for the `root` user. Ignored if existing secret is provided
    ## ref: https://github.com/bitnami/bitnami-docker-mysql#setting-the-root-password-on-first-run
    ##
    rootPassword: 'rootPassword'
    ## @param auth.database Name for a custom database to create
    ## ref: https://github.com/bitnami/bitnami-docker-mysql/blob/master/README.md#creating-a-database-on-first-run
    ##
    database: keto
    ## @param auth.username Name for a custom user to create
    ## ref: https://github.com/bitnami/bitnami-docker-mysql/blob/master/README.md#creating-a-database-user-on-first-run
    ##
    username: 'user'
    ## @param auth.password Password for the new user. Ignored if existing secret is provided
    ##
    password: 'password'
    ## @param auth.replicationUser MySQL replication user
    ## ref: https://github.com/bitnami/bitnami-docker-mysql#setting-up-a-replication-cluster
    ##
    replicationUser: replicator
    ## @param auth.replicationPassword MySQL replication user password. Ignored if existing secret is provided
    ##
    replicationPassword: ''
    ## @param auth.existingSecret Use existing secret for password details. The secret has to contain the keys `mysql-root-password`, `mysql-replication-password` and `mysql-password`
    ## NOTE: When it's set the auth.rootPassword, auth.password, auth.replicationPassword are ignored.
    ##
    existingSecret: ''
    ## @param auth.forcePassword Force users to specify required passwords
    ##
    forcePassword: true
    ## @param auth.usePasswordFiles Mount credentials as files instead of using an environment variable
    ##
    usePasswordFiles: false
    ## @param auth.customPasswordFiles [object] Use custom password files when `auth.usePasswordFiles` is set to `true`. Define path for keys `root` and `user`, also define `replicator` if `architecture` is set to `replication`
    ## Example:
    ## customPasswordFiles:
    ##   root: /vault/secrets/mysql-root
    ##   user: /vault/secrets/mysql-user
    ##   replicator: /vault/secrets/mysql-replicator
    ##
    customPasswordFiles: {}
  ## @param initdbScripts [object] Dictionary of initdb scripts
  ## Specify dictionary of scripts to be run at first boot
  ## Example:
  ## initdbScripts:
  ##   my_init_script.sh: |
  ##      #!/bin/bash
  ##      echo "Do something."
  ##
  # initdbScripts: {}
  initdbScripts:
    # This script enables legacy authentication for MySQL v8. NodeJS MySQL Client currently does not support authentication plugins, reference: https://github.com/mysqljs/mysql/pull/2233
    enableLegacyAuth.sql: |-
      ALTER USER 'user'@'%' IDENTIFIED WITH mysql_native_password BY 'password';
  ## @param initdbScriptsConfigMap ConfigMap with the initdb scripts (Note: Overrides `initdbScripts`)
  ##
  initdbScriptsConfigMap: ''

  ## @section MySQL Primary parameters

  primary:
    ## @param primary.command [array] Override default container command on MySQL Primary container(s) (useful when using custom images)
    ##
    command: []
    ## @param primary.args [array] Override default container args on MySQL Primary container(s) (useful when using custom images)
    ##
    args: []
    ## @param primary.hostAliases [array] Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param primary.configuration [string] Configure MySQL Primary with a custom my.cnf file
    ## ref: https://mysql.com/kb/en/mysql/configuring-mysql-with-mycnf/#example-of-configuration-file
    ##
    configuration: |-
      [mysqld]
      default_authentication_plugin=mysql_native_password
      skip-name-resolve
      explicit_defaults_for_timestamp
      basedir=/opt/bitnami/mysql
      plugin_dir=/opt/bitnami/mysql/lib/plugin
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      datadir=/bitnami/mysql/data
      tmpdir=/opt/bitnami/mysql/tmp
      max_allowed_packet=16M
      bind-address=0.0.0.0
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
      log-error=/opt/bitnami/mysql/logs/mysqld.log
      character-set-server=UTF8
      collation-server=utf8_general_ci

      [client]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      default-character-set=UTF8
      plugin_dir=/opt/bitnami/mysql/lib/plugin

      [manager]
      port=3306
      socket=/opt/bitnami/mysql/tmp/mysql.sock
      pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    ## @param primary.existingConfiguration Name of existing ConfigMap with MySQL Primary configuration.
    ## NOTE: When it's set the 'configuration' parameter is ignored
    ##
    existingConfiguration: ''
    ## @param primary.updateStrategy Update strategy type for the MySQL primary statefulset
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy: RollingUpdate
    ## @param primary.rollingUpdatePartition Partition update strategy for MySQL Primary statefulset
    ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
    ##
    rollingUpdatePartition: ''
    ## @param primary.podAnnotations [object] Additional pod annotations for MySQL primary pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param primary.podAffinityPreset MySQL primary pod affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ''
    ## @param primary.podAntiAffinityPreset MySQL primary pod anti-affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## MySQL Primary node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param primary.nodeAffinityPreset.type MySQL primary node affinity preset type. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ''
      ## @param primary.nodeAffinityPreset.key MySQL primary node label key to match Ignored if `primary.affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ''
      ## @param primary.nodeAffinityPreset.values [array] MySQL primary node label values to match. Ignored if `primary.affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param primary.affinity [object] Affinity for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param primary.nodeSelector [object] Node labels for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}
    ## @param primary.tolerations [array] Tolerations for MySQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## MySQL primary Pod security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param primary.podSecurityContext.enabled Enable security context for MySQL primary pods
    ## @param primary.podSecurityContext.fsGroup Group ID for the mounted volumes' filesystem
    ##
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    ## MySQL primary container security context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param primary.containerSecurityContext.enabled MySQL primary container securityContext
    ## @param primary.containerSecurityContext.runAsUser User ID for the MySQL primary container
    ##
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    ## MySQL primary container's resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param primary.resources.limits [object] The resources limits for MySQL primary containers
    ## @param primary.resources.requests [object] The requested resources for MySQL primary containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 250m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 250m
      ##    memory: 256Mi
      requests: {}
    ## Configure extra options for liveness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.livenessProbe.enabled Enable livenessProbe
    ## @param primary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param primary.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param primary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param primary.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param primary.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for readiness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.readinessProbe.enabled Enable readinessProbe
    ## @param primary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param primary.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param primary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param primary.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param primary.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1
    ## Configure extra options for startupProbe probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param primary.startupProbe.enabled Enable startupProbe
    ## @param primary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param primary.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param primary.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param primary.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param primary.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: true
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1
    ## @param primary.customLivenessProbe [object] Override default liveness probe for MySQL primary containers
    ##
    customLivenessProbe: {}
    ## @param primary.customReadinessProbe [object] Override default readiness probe for MySQL primary containers
    ##
    customReadinessProbe: {}
    ## @param primary.customStartupProbe [object] Override default startup probe for MySQL primary containers
    ##
    customStartupProbe: {}
    ## @param primary.extraFlags MySQL primary additional command line flags
    ## Can be used to specify command line flags, for example:
    ## E.g.
    ## extraFlags: "--max-connect-errors=1000 --max_connections=155"
    ##
    extraFlags: ''
    ## @param primary.extraEnvVars [array] Extra environment variables to be set on MySQL primary containers
    ## E.g.
    ## extraEnvVars:
    ##  - name: TZ
    ##    value: "Europe/Paris"
    ##
    extraEnvVars: []
    ## @param primary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for MySQL primary containers
    ##
    extraEnvVarsCM: ''
    ## @param primary.extraEnvVarsSecret Name of existing Secret containing extra env vars for MySQL primary containers
    ##
    extraEnvVarsSecret: ''
    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      ## @param primary.persistence.enabled Enable persistence on MySQL primary replicas using a `PersistentVolumeClaim`. If false, use emptyDir
      ##
      enabled: false
      ## @param primary.persistence.existingClaim Name of an existing `PersistentVolumeClaim` for MySQL primary replicas
      ## NOTE: When it's set the rest of persistence parameters are ignored
      ##
      existingClaim: ''
      ## @param primary.persistence.storageClass MySQL primary persistent volume storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ''
      ## @param primary.persistence.annotations [object] MySQL primary persistent volume claim annotations
      ##
      annotations: {}
      ## @param primary.persistence.accessModes MySQL primary persistent volume access Modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param primary.persistence.size MySQL primary persistent volume size
      ##
      size: 8Gi
      ## @param primary.persistence.selector [object] Selector to match an existing Persistent Volume
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @param primary.extraVolumes [array] Optionally specify extra list of additional volumes to the MySQL Primary pod(s)
    ##
    extraVolumes: []
    ## @param primary.extraVolumeMounts [array] Optionally specify extra list of additional volumeMounts for the MySQL Primary container(s)
    ##
    extraVolumeMounts: []
    ## @param primary.initContainers [array] Add additional init containers for the MySQL Primary pod(s)
    ##
    initContainers: []
    ## @param primary.sidecars [array] Add additional sidecar containers for the MySQL Primary pod(s)
    ##
    sidecars: []
    ## MySQL Primary Service parameters
    ##
    service:
      ## @param primary.service.type MySQL Primary K8s service type
      ##
      type: ClusterIP
      ## @param primary.service.port MySQL Primary K8s service port
      ##
      port: 3306
      ## @param primary.service.nodePort MySQL Primary K8s service node port
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePort: ''
      ## @param primary.service.clusterIP MySQL Primary K8s service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ''
      ## @param primary.service.loadBalancerIP MySQL Primary loadBalancerIP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ''
      ## @param primary.service.externalTrafficPolicy Enable client source IP preservation
      ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param primary.service.loadBalancerSourceRanges [array] Addresses that are allowed when MySQL Primary service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## E.g.
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param primary.service.annotations [object] Provide any additional annotations which may be required
      ##
      annotations: {}
    ## MySQL primary Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      ## @param primary.pdb.enabled Enable/disable a Pod Disruption Budget creation for MySQL primary pods
      ##
      enabled: false
      ## @param primary.pdb.minAvailable Minimum number/percentage of MySQL primary pods that should remain scheduled
      ##
      minAvailable: 1
      ## @param primary.pdb.maxUnavailable Maximum number/percentage of MySQL primary pods that may be made unavailable
      ##
      maxUnavailable: ''
    ## @param primary.podLabels [object] MySQL Primary pod label. If labels are same as commonLabels , this will take precedence
    ##
    podLabels: {}

  ## @section RBAC parameters

  ## MySQL pods ServiceAccount
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable the creation of a ServiceAccount for MySQL pods
    ##
    create: true
    ## @param serviceAccount.name Name of the created ServiceAccount
    ## If not set and create is true, a name is generated using the mysql.fullname template
    ##
    name: ''
    ## @param serviceAccount.annotations [object] Annotations for MySQL Service Account
    ##
    annotations: {}
  ## Role Based Access
  ## ref: https://kubernetes.io/docs/admin/authorization/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ##
    create: false

  ## @section Network Policy

  ## MySQL Nework Policy configuration
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: false
    ## @param networkPolicy.allowExternal The Policy model to apply.
    ## When set to false, only pods with the correct
    ## client label will have network access to the port MySQL is listening
    ## on. When true, MySQL will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.explicitNamespacesSelector [object] A Kubernetes LabelSelector to explicitly select namespaces from which ingress traffic could be allowed to MySQL
    ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace
    ## and that match other criteria, the ones that have the good label, can reach the DB.
    ## But sometimes, we want the DB to be accessible to clients from other namespaces, in this case, we can use this
    ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.
    ##
    ## Example:
    ## explicitNamespacesSelector:
    ##   matchLabels:
    ##     role: frontend
    ##   matchExpressions:
    ##    - {key: role, operator: In, values: [frontend]}
    ##
    explicitNamespacesSelector: {}

  ## @section Volume Permissions parameters

  ## Init containers parameters:
  ## volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`
    ##
    enabled: false
    ## @param volumePermissions.image.registry Init container volume-permissions image registry
    ## @param volumePermissions.image.repository Init container volume-permissions image repository
    ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets [array] Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r172
      pullPolicy: Always
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## @param volumePermissions.resources [object] Init container volume-permissions resources
    ##
    resources: {}

  ## @section Metrics parameters

  ## Mysqld Prometheus exporter parameters
  ##
  metrics:
    ## @param metrics.enabled Start a side-car prometheus exporter
    ##
    enabled: false
    ## @param metrics.image.registry Exporter image registry
    ## @param metrics.image.repository Exporter image repository
    ## @param metrics.image.tag Exporter image tag (immutable tags are recommended)
    ## @param metrics.image.pullPolicy Exporter image pull policy
    ## @param metrics.image.pullSecrets [array] Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/mysqld-exporter
      tag: 0.13.0-debian-10-r75
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## MySQL Prometheus exporter service parameters
    ## Mysqld Prometheus exporter liveness and readiness probes
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.service.type Kubernetes service type for MySQL Prometheus Exporter
    ## @param metrics.service.port MySQL Prometheus Exporter service port
    ## @param metrics.service.annotations [object] Prometheus exporter service annotations
    ##
    service:
      type: ClusterIP
      port: 9104
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
    ## @param metrics.extraArgs.primary [array] Extra args to be passed to mysqld_exporter on Primary pods
    ## @param metrics.extraArgs.secondary [array] Extra args to be passed to mysqld_exporter on Secondary pods
    ## ref: https://github.com/prometheus/mysqld_exporter/
    ## E.g.
    ## - --collect.auto_increment.columns
    ## - --collect.binlog_size
    ## - --collect.engine_innodb_status
    ## - --collect.engine_tokudb_status
    ## - --collect.global_status
    ## - --collect.global_variables
    ## - --collect.info_schema.clientstats
    ## - --collect.info_schema.innodb_metrics
    ## - --collect.info_schema.innodb_tablespaces
    ## - --collect.info_schema.innodb_cmp
    ## - --collect.info_schema.innodb_cmpmem
    ## - --collect.info_schema.processlist
    ## - --collect.info_schema.processlist.min_time
    ## - --collect.info_schema.query_response_time
    ## - --collect.info_schema.tables
    ## - --collect.info_schema.tables.databases
    ## - --collect.info_schema.tablestats
    ## - --collect.info_schema.userstats
    ## - --collect.perf_schema.eventsstatements
    ## - --collect.perf_schema.eventsstatements.digest_text_limit
    ## - --collect.perf_schema.eventsstatements.limit
    ## - --collect.perf_schema.eventsstatements.timelimit
    ## - --collect.perf_schema.eventswaits
    ## - --collect.perf_schema.file_events
    ## - --collect.perf_schema.file_instances
    ## - --collect.perf_schema.indexiowaits
    ## - --collect.perf_schema.tableiowaits
    ## - --collect.perf_schema.tablelocks
    ## - --collect.perf_schema.replication_group_member_stats
    ## - --collect.slave_status
    ## - --collect.slave_hosts
    ## - --collect.heartbeat
    ## - --collect.heartbeat.database
    ## - --collect.heartbeat.table
    ##
    extraArgs:
      primary: []
      secondary: []
    ## Mysqld Prometheus exporter resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param metrics.resources.limits [object] The resources limits for MySQL prometheus exporter containers
    ## @param metrics.resources.requests [object] The requested resources for MySQL prometheus exporter containers
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 100m
      ##    memory: 256Mi
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 100m
      ##    memory: 256Mi
      requests: {}
    ## Mysqld Prometheus exporter liveness probe
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.livenessProbe.enabled Enable livenessProbe
    ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 120
      periodSeconds: 10
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 3
    ## Mysqld Prometheus exporter readiness probe
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param metrics.readinessProbe.enabled Enable readinessProbe
    ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 3
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator
      ##
      enabled: true
      ## @param metrics.serviceMonitor.namespace Specify the namespace in which the serviceMonitor resource will be created
      ##
      namespace: ''
      ## @param metrics.serviceMonitor.interval Specify the interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
      ## e.g:
      ## scrapeTimeout: 30s
      ##
      scrapeTimeout: ''
      ## @param metrics.serviceMonitor.relabellings [array] Specify Metric Relabellings to add to the scrape endpoint
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels [object] Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      additionalLabels: {}

################################################################
## WSO2 Identity Server external configuration
## Currently the wso2 identity server is not included in this helm chart
## There is an example provided here to run a wso2 identity server using docker-compose method for now
## TODO: helm chart of the wso2 is to be included here
wso2:
  identityServer:
    createSecrets: true
    secrets:
      wso2-is-admin-creds:
        password: 'admin'
    #      - chart-example.local
